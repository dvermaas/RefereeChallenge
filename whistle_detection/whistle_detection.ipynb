{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs & Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dverm\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Extracting Features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import IPython\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# Training neural networks\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Live detection\n",
    "import pyaudio\n",
    "from IPython.display import clear_output\n",
    "import wave\n",
    "\n",
    "# ms per chunk\n",
    "STEP = 50\n",
    "PATH = \"whistle_dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The chunk processor will get a sample with a feature type, and will utilize these to process said chunk into features. The chunk processor is used in both featurizers, but with different window values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_processor(sample, sample_rate, feature_type, target, true_window, false_window):\n",
    "    \n",
    "    # get correct window variable\n",
    "    if target:\n",
    "        window = true_window\n",
    "    else:\n",
    "        window = false_window\n",
    "        \n",
    "    # calculate chunk size\n",
    "    chunk = int((sample_rate / 1000) * STEP)\n",
    "    \n",
    "    # iterate over sample and fetch features\n",
    "    for i in tqdm(range(0, len(sample) - chunk, int(chunk * window)), leave=False):\n",
    "        if feature_type == \"fft\":\n",
    "            chunk_features = np.mean(np.abs(librosa.stft(sample[i:i+chunk], n_fft=512, hop_length=256, win_length=512)).T, axis=0)\n",
    "        elif feature_type == \"mfcc\":\n",
    "            chunk_features = np.mean(librosa.feature.mfcc(y=sample[i:i+chunk], sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        \n",
    "        try:\n",
    "            features = np.append(features, np.array([chunk_features]), axis=0)\n",
    "        except:\n",
    "            features = np.array([chunk_features])\n",
    "            \n",
    "    return features, np.full(len(features), int(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizer V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Featurizer 1 only uses the whistle, and the segments before and after (same length). This is how a 2:1 label ratio is managed. Sequential samples also have 80% overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old windows were 1/5\n",
    "def build_feature_dataframe_v1(feature_type, denoise=False, true_window=1/5):\n",
    "    \"\"\"Convert all whistles and small fragments before and after said whistles into features\"\"\"\n",
    "    \n",
    "    # See if csv has been calculated before (saving time)\n",
    "    try:\n",
    "        df = pd.read_csv(\"data_v1_\" + feature_type + \"_\" + str(denoise) + \".csv\", index_col=0)\n",
    "        print(\"Dataframe succesfully loaded from csv!\")\n",
    "        return df\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get the labels\n",
    "    target = []\n",
    "    with open(PATH + \"whistledb.json\") as json_file:\n",
    "        labels = json.load(json_file)[\"audioFiles\"]\n",
    "        labels = {entry[\"path\"] : entry[\"channels\"][1][\"whistleLabels\"] for entry in labels}\n",
    "    \n",
    "    # iterate over all audiofiles\n",
    "    for file_name in tqdm(os.listdir(PATH)):\n",
    "        # skip json file\n",
    "        if file_name.split(\".\")[-1] != \"wav\":\n",
    "            continue\n",
    "\n",
    "        # load file & meta data\n",
    "        sample, sample_rate = librosa.load(PATH + file_name, sr=None)\n",
    "        if denoise == True:\n",
    "            sample = nr.reduce_noise(y=sample,  y_noise=sample[0:5000], sr=sample_rate)\n",
    "\n",
    "        # for all positive intervals get part before and after aswell and featurize\n",
    "        for times in tqdm(labels[file_name], leave=False):\n",
    "            delta_time = times[\"end\"] - times[\"start\"]\n",
    "            \n",
    "            label = False\n",
    "            for i in range(times[\"start\"]-delta_time, times[\"end\"]+delta_time, delta_time):\n",
    "                if i >= 0 and i + delta_time <= len(sample):\n",
    "                    features, targets = chunk_processor(sample[i:i+delta_time], sample_rate, feature_type, label, \n",
    "                                                        true_window, true_window)\n",
    "                    label = not(label)\n",
    "                    try:\n",
    "                        out = np.append(out, features, axis=0)\n",
    "                        target = np.append(target, targets)\n",
    "                    except:\n",
    "                        out = features\n",
    "                        target = targets\n",
    "                else:\n",
    "                    print(file_name, i, 'no fit ;(')\n",
    "\n",
    "    # save them in dataframe\n",
    "    df = pd.DataFrame(out)\n",
    "    df=(df-df.min())/(df.max()-df.min())\n",
    "    df.insert(0, \"target\", target)\n",
    "    df.to_csv(\"data_v1_\" + feature_type + \"_\" + str(denoise) + \".csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizer V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Featurizer 2 utilises two different overlap values. True labels have 1/5 overlap, meaning that sequential samples have 80% overlap. False labels have a overlap value of 10, meaning that after each sample taken 9 are skipped. This insures that a 2:1 label ratio is kept while still taking a more fair representation of the dataset. True windows of 1/2 and 1/50 can also be used with succes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_dataframe_v2(feature_type, denoise=False, true_window=1/5):\n",
    "    false_window = int(STEP * true_window)\n",
    "    \n",
    "    # See if csv has been calculated before (saving time)\n",
    "    try:\n",
    "        df = pd.read_csv(\"data_v2_\" + feature_type + \"_\" + str(denoise) + \".csv\", index_col=0)\n",
    "        print(\"Dataframe succesfully loaded from csv!\")\n",
    "        return df\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get the labels\n",
    "    target = []\n",
    "    with open(PATH + \"whistledb.json\") as json_file:\n",
    "        labels = json.load(json_file)[\"audioFiles\"]\n",
    "        labels = {entry[\"path\"] : entry[\"channels\"][1][\"whistleLabels\"] for entry in labels}\n",
    "    \n",
    "    # iterate over all audiofiles\n",
    "    for file_name in tqdm(os.listdir(PATH)):\n",
    "        # skip json file\n",
    "        if file_name.split(\".\")[-1] != \"wav\":\n",
    "            continue\n",
    "\n",
    "        # load file & meta data\n",
    "        sample, sample_rate = librosa.load(PATH + file_name, sr=None)\n",
    "        if denoise == True:\n",
    "            sample = nr.reduce_noise(y=sample,  y_noise=sample[0:5000], sr=sample_rate)\n",
    "        \n",
    "        # create time intervals\n",
    "        times_list = [0]\n",
    "        for times in labels[file_name]:\n",
    "            times_list += [times[\"start\"], times[\"end\"]]\n",
    "        if times_list[-1] < len(sample):\n",
    "            times_list.append(len(sample))\n",
    "        \n",
    "        label = False\n",
    "        for i in tqdm(range(len(times_list)-1), leave=False):\n",
    "            features, targets = chunk_processor(sample[times_list[i]:times_list[i+1]], sample_rate, feature_type, label, \n",
    "                                                true_window, false_window)\n",
    "            label = not(label)\n",
    "            try:\n",
    "                out = np.append(out, features, axis=0)\n",
    "                target = np.append(target, targets)\n",
    "            except:\n",
    "                out = features\n",
    "                target = targets\n",
    "\n",
    "    # save them in dataframe\n",
    "    df = pd.DataFrame(out)\n",
    "    df=(df-df.min())/(df.max()-df.min())\n",
    "    df.insert(0, \"target\", target)\n",
    "    df.to_csv(\"data_v2_\" + feature_type + \"_\" + str(denoise) + \".csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(version, feature_type, denoise):\n",
    "    if version == 1:\n",
    "        return build_feature_dataframe_v1(feature_type, denoise)\n",
    "    elif version == 2:\n",
    "        return build_feature_dataframe_v2(feature_type, denoise)\n",
    "    else:\n",
    "        print(\"invalid version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe succesfully loaded from csv!\n",
      "True label ratio: 33.5 %\n"
     ]
    }
   ],
   "source": [
    "mass_data = build_features(1, \"mfcc\", False)\n",
    "print(\"True label ratio:\", round((len(mass_data[mass_data[\"target\"] == 1]) / len(mass_data[\"target\"])) * 100, 1),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following code allows for any whistle sample to be extracted in order to check if they are labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhistleTest_tuhhnao16.wav\n",
      "[{'start': 14930000, 'end': 14970000}, {'start': 19186000, 'end': 19250000}, {'start': 19840000, 'end': 19930000}, {'start': 20339000, 'end': 20400000}, {'start': 20918000, 'end': 20955000}, {'start': 21082000, 'end': 21140000}, {'start': 21463000, 'end': 21520000}, {'start': 23044000, 'end': 23110000}, {'start': 23676000, 'end': 23730000}, {'start': 24353000, 'end': 24410000}, {'start': 27023000, 'end': 27060000}, {'start': 27830000, 'end': 27880000}, {'start': 28075000, 'end': 28116000}] 13\n"
     ]
    }
   ],
   "source": [
    "# Choose recording\n",
    "i = -1\n",
    "fname = os.listdir(PATH)[i]\n",
    "print(fname)\n",
    "\n",
    "with open(PATH + \"whistledb.json\") as json_file:\n",
    "    labels = json.load(json_file)[\"audioFiles\"]\n",
    "    labels = {entry[\"path\"] : entry[\"channels\"][1][\"whistleLabels\"] for entry in labels}\n",
    "print(labels[fname], len(labels[fname]))\n",
    "\n",
    "sample_tts, sr = librosa.load(PATH + fname, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HTWK_Dutch_05_04_2017-2.wav', 'HTWK_Dutch_05_04_2017.wav', 'HTWK_HULKs_1930_2016_04_29.wav', 'Lab_Fox40_vs_printed.wav', 'LUnitedvsUPenn20170728.wav', 'LUnitedvsUPenn20170728Half2.wav', 'SRC_Dutch_05-04-2017.wav', 'TestBHuman_03_05_2017_1413.wav', 'whistledb.json', 'WhistleTest_tuhhnao12.wav', 'WhistleTest_tuhhnao16.wav']\n",
      "28075000 28116000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose whistle number\n",
    "i = -1\n",
    "print(os.listdir(PATH))\n",
    "print(labels[fname][i][\"start\"],labels[fname][i][\"end\"])\n",
    "\n",
    "IPython.display.Audio(sample_tts[labels[fname][i][\"start\"]:labels[fname][i][\"end\"]], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomise and split dataframe into X and Y\n",
    "def feature_target_split(df, shuffle=True):\n",
    "    if shuffle:\n",
    "        df = df.sample(frac = 1)\n",
    "    \n",
    "    dataset = df.values\n",
    "    X = dataset[:,1:].astype(float)\n",
    "    \n",
    "    Y = dataset[:,0]\n",
    "    encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    Y = encoder.transform(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_integrated(X):\n",
    "    inputs = Input(shape= (X.shape[1],))\n",
    "    layer = Dense(128, activation=\"relu\")(inputs)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(layer)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss = \"binary_crossentropy\",optimizer = \"adam\",metrics = [\"acc\"])\n",
    "    mc = ModelCheckpoint(\"best_model_whistle.hdf5\", monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    return model, mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe succesfully loaded from csv!\n",
      "Train on 12012 samples, validate on 3003 samples\n",
      "Epoch 1/200\n",
      "12012/12012 [==============================] - 1s 82us/step - loss: 0.3108 - acc: 0.8896 - val_loss: 0.2079 - val_acc: 0.9121\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20790, saving model to best_model_whistle.hdf5\n",
      "Epoch 2/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.1621 - acc: 0.9418 - val_loss: 0.1545 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20790 to 0.15446, saving model to best_model_whistle.hdf5\n",
      "Epoch 3/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.1308 - acc: 0.9531 - val_loss: 0.1375 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15446 to 0.13750, saving model to best_model_whistle.hdf5\n",
      "Epoch 4/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.1160 - acc: 0.9581 - val_loss: 0.1296 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13750 to 0.12957, saving model to best_model_whistle.hdf5\n",
      "Epoch 5/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.1081 - acc: 0.9613 - val_loss: 0.1222 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12957 to 0.12224, saving model to best_model_whistle.hdf5\n",
      "Epoch 6/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.1004 - acc: 0.9644 - val_loss: 0.1180 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12224 to 0.11803, saving model to best_model_whistle.hdf5\n",
      "Epoch 7/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0964 - acc: 0.9651 - val_loss: 0.1085 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11803 to 0.10854, saving model to best_model_whistle.hdf5\n",
      "Epoch 8/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0912 - acc: 0.9669 - val_loss: 0.1048 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10854 to 0.10481, saving model to best_model_whistle.hdf5\n",
      "Epoch 9/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0875 - acc: 0.9677 - val_loss: 0.0988 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10481 to 0.09882, saving model to best_model_whistle.hdf5\n",
      "Epoch 10/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0828 - acc: 0.9704 - val_loss: 0.0940 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09882 to 0.09395, saving model to best_model_whistle.hdf5\n",
      "Epoch 11/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0803 - acc: 0.9699 - val_loss: 0.0913 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09395 to 0.09126, saving model to best_model_whistle.hdf5\n",
      "Epoch 12/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0771 - acc: 0.9718 - val_loss: 0.0923 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09126\n",
      "Epoch 13/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0736 - acc: 0.9734 - val_loss: 0.0872 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09126 to 0.08715, saving model to best_model_whistle.hdf5\n",
      "Epoch 14/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0707 - acc: 0.9744 - val_loss: 0.0839 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08715 to 0.08390, saving model to best_model_whistle.hdf5\n",
      "Epoch 15/200\n",
      "12012/12012 [==============================] - 1s 57us/step - loss: 0.0699 - acc: 0.9743 - val_loss: 0.0798 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08390 to 0.07980, saving model to best_model_whistle.hdf5\n",
      "Epoch 16/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0660 - acc: 0.9762 - val_loss: 0.0755 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07980 to 0.07552, saving model to best_model_whistle.hdf5\n",
      "Epoch 17/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0643 - acc: 0.9772 - val_loss: 0.0747 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07552 to 0.07471, saving model to best_model_whistle.hdf5\n",
      "Epoch 18/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0619 - acc: 0.9770 - val_loss: 0.0693 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07471 to 0.06926, saving model to best_model_whistle.hdf5\n",
      "Epoch 19/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0587 - acc: 0.9775 - val_loss: 0.0671 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06926 to 0.06713, saving model to best_model_whistle.hdf5\n",
      "Epoch 20/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0569 - acc: 0.9794 - val_loss: 0.0680 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06713\n",
      "Epoch 21/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0560 - acc: 0.9789 - val_loss: 0.0631 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06713 to 0.06314, saving model to best_model_whistle.hdf5\n",
      "Epoch 22/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0544 - acc: 0.9800 - val_loss: 0.0634 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06314\n",
      "Epoch 23/200\n",
      "12012/12012 [==============================] - 1s 56us/step - loss: 0.0522 - acc: 0.9807 - val_loss: 0.0603 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06314 to 0.06025, saving model to best_model_whistle.hdf5\n",
      "Epoch 24/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0509 - acc: 0.9804 - val_loss: 0.0605 - val_acc: 0.9767\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06025\n",
      "Epoch 25/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0496 - acc: 0.9819 - val_loss: 0.0575 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06025 to 0.05753, saving model to best_model_whistle.hdf5\n",
      "Epoch 26/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0483 - acc: 0.9831 - val_loss: 0.0558 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05753 to 0.05583, saving model to best_model_whistle.hdf5\n",
      "Epoch 27/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0565 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05583\n",
      "Epoch 28/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0463 - acc: 0.9836 - val_loss: 0.0576 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05583\n",
      "Epoch 29/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0449 - acc: 0.9838 - val_loss: 0.0540 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05583 to 0.05399, saving model to best_model_whistle.hdf5\n",
      "Epoch 30/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0428 - acc: 0.9841 - val_loss: 0.0508 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05399 to 0.05082, saving model to best_model_whistle.hdf5\n",
      "Epoch 31/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0421 - acc: 0.9849 - val_loss: 0.0494 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05082 to 0.04945, saving model to best_model_whistle.hdf5\n",
      "Epoch 32/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0426 - acc: 0.9846 - val_loss: 0.0521 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04945\n",
      "Epoch 33/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.0487 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04945 to 0.04865, saving model to best_model_whistle.hdf5\n",
      "Epoch 34/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0396 - acc: 0.9853 - val_loss: 0.0472 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04865 to 0.04716, saving model to best_model_whistle.hdf5\n",
      "Epoch 35/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0395 - acc: 0.9860 - val_loss: 0.0504 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.04716\n",
      "Epoch 36/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0382 - acc: 0.9858 - val_loss: 0.0456 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04716 to 0.04559, saving model to best_model_whistle.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0383 - acc: 0.9855 - val_loss: 0.0452 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04559 to 0.04520, saving model to best_model_whistle.hdf5\n",
      "Epoch 38/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0373 - acc: 0.9850 - val_loss: 0.0477 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04520\n",
      "Epoch 39/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0362 - acc: 0.9868 - val_loss: 0.0429 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04520 to 0.04291, saving model to best_model_whistle.hdf5\n",
      "Epoch 40/200\n",
      "12012/12012 [==============================] - 1s 57us/step - loss: 0.0353 - acc: 0.9868 - val_loss: 0.0441 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.04291\n",
      "Epoch 41/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0353 - acc: 0.9866 - val_loss: 0.0430 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.04291\n",
      "Epoch 42/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0355 - acc: 0.9873 - val_loss: 0.0418 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04291 to 0.04183, saving model to best_model_whistle.hdf5\n",
      "Epoch 43/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0338 - acc: 0.9873 - val_loss: 0.0404 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04183 to 0.04045, saving model to best_model_whistle.hdf5\n",
      "Epoch 44/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0327 - acc: 0.9879 - val_loss: 0.0404 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04045 to 0.04036, saving model to best_model_whistle.hdf5\n",
      "Epoch 45/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0324 - acc: 0.9875 - val_loss: 0.0396 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.04036 to 0.03964, saving model to best_model_whistle.hdf5\n",
      "Epoch 46/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0322 - acc: 0.9868 - val_loss: 0.0409 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03964\n",
      "Epoch 47/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0307 - acc: 0.9888 - val_loss: 0.0381 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03964 to 0.03813, saving model to best_model_whistle.hdf5\n",
      "Epoch 48/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0309 - acc: 0.9878 - val_loss: 0.0431 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03813\n",
      "Epoch 49/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0313 - acc: 0.9879 - val_loss: 0.0386 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03813\n",
      "Epoch 50/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0297 - acc: 0.9887 - val_loss: 0.0371 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03813 to 0.03714, saving model to best_model_whistle.hdf5\n",
      "Epoch 51/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0286 - acc: 0.9894 - val_loss: 0.0370 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03714 to 0.03699, saving model to best_model_whistle.hdf5\n",
      "Epoch 52/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0286 - acc: 0.9883 - val_loss: 0.0391 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03699\n",
      "Epoch 53/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0286 - acc: 0.9887 - val_loss: 0.0361 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03699 to 0.03609, saving model to best_model_whistle.hdf5\n",
      "Epoch 54/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0279 - acc: 0.9893 - val_loss: 0.0343 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03609 to 0.03435, saving model to best_model_whistle.hdf5\n",
      "Epoch 55/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0286 - acc: 0.9894 - val_loss: 0.0357 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03435\n",
      "Epoch 56/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0270 - acc: 0.9897 - val_loss: 0.0340 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.03435 to 0.03403, saving model to best_model_whistle.hdf5\n",
      "Epoch 57/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0257 - acc: 0.9902 - val_loss: 0.0353 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03403\n",
      "Epoch 58/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0259 - acc: 0.9900 - val_loss: 0.0369 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03403\n",
      "Epoch 59/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0274 - acc: 0.9895 - val_loss: 0.0393 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03403\n",
      "Epoch 60/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0251 - acc: 0.9898 - val_loss: 0.0338 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03403 to 0.03383, saving model to best_model_whistle.hdf5\n",
      "Epoch 61/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0246 - acc: 0.9907 - val_loss: 0.0326 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03383 to 0.03263, saving model to best_model_whistle.hdf5\n",
      "Epoch 62/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0243 - acc: 0.9906 - val_loss: 0.0371 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03263\n",
      "Epoch 63/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0245 - acc: 0.9911 - val_loss: 0.0326 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.03263 to 0.03257, saving model to best_model_whistle.hdf5\n",
      "Epoch 64/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0246 - acc: 0.9905 - val_loss: 0.0309 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.03257 to 0.03094, saving model to best_model_whistle.hdf5\n",
      "Epoch 65/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0227 - acc: 0.9913 - val_loss: 0.0342 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03094\n",
      "Epoch 66/200\n",
      "12012/12012 [==============================] - 1s 63us/step - loss: 0.0233 - acc: 0.9909 - val_loss: 0.0388 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03094\n",
      "Epoch 67/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0228 - acc: 0.9910 - val_loss: 0.0304 - val_acc: 0.98670s - loss: 0.0219 - acc:\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03094 to 0.03039, saving model to best_model_whistle.hdf5\n",
      "Epoch 68/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0226 - acc: 0.9911 - val_loss: 0.0299 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03039 to 0.02991, saving model to best_model_whistle.hdf5\n",
      "Epoch 69/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0229 - acc: 0.9912 - val_loss: 0.0311 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02991\n",
      "Epoch 70/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0214 - acc: 0.9926 - val_loss: 0.0306 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02991\n",
      "Epoch 71/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0211 - acc: 0.9920 - val_loss: 0.0315 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02991\n",
      "Epoch 72/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0287 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.02991 to 0.02866, saving model to best_model_whistle.hdf5\n",
      "Epoch 73/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0224 - acc: 0.9918 - val_loss: 0.0286 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.02866 to 0.02863, saving model to best_model_whistle.hdf5\n",
      "Epoch 74/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0205 - acc: 0.9925 - val_loss: 0.0286 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02863\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0205 - acc: 0.9928 - val_loss: 0.0304 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02863\n",
      "Epoch 76/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0203 - acc: 0.9928 - val_loss: 0.0271 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.02863 to 0.02712, saving model to best_model_whistle.hdf5\n",
      "Epoch 77/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0193 - acc: 0.9931 - val_loss: 0.0275 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02712\n",
      "Epoch 78/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0188 - acc: 0.9936 - val_loss: 0.0273 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02712\n",
      "Epoch 79/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0268 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.02712 to 0.02685, saving model to best_model_whistle.hdf5\n",
      "Epoch 80/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0198 - acc: 0.9932 - val_loss: 0.0289 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02685\n",
      "Epoch 81/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0273 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02685\n",
      "Epoch 82/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.0267 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.02685 to 0.02671, saving model to best_model_whistle.hdf5\n",
      "Epoch 83/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0178 - acc: 0.9938 - val_loss: 0.0254 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.02671 to 0.02543, saving model to best_model_whistle.hdf5\n",
      "Epoch 84/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02543\n",
      "Epoch 85/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0180 - acc: 0.9938 - val_loss: 0.0263 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02543\n",
      "Epoch 86/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0173 - acc: 0.9943 - val_loss: 0.0297 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02543\n",
      "Epoch 87/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0293 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02543\n",
      "Epoch 88/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0265 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02543\n",
      "Epoch 89/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0185 - acc: 0.9932 - val_loss: 0.0282 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02543\n",
      "Epoch 90/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 0.0268 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02543\n",
      "Epoch 91/200\n",
      "12012/12012 [==============================] - 1s 57us/step - loss: 0.0160 - acc: 0.9946 - val_loss: 0.0237 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02543 to 0.02373, saving model to best_model_whistle.hdf5\n",
      "Epoch 92/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.0242 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02373\n",
      "Epoch 93/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0236 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.02373 to 0.02356, saving model to best_model_whistle.hdf5\n",
      "Epoch 94/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0157 - acc: 0.9944 - val_loss: 0.0254 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02356\n",
      "Epoch 95/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0152 - acc: 0.9955 - val_loss: 0.0266 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02356\n",
      "Epoch 96/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0153 - acc: 0.9953 - val_loss: 0.0235 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.02356 to 0.02355, saving model to best_model_whistle.hdf5\n",
      "Epoch 97/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0157 - acc: 0.9946 - val_loss: 0.0224 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02355 to 0.02241, saving model to best_model_whistle.hdf5\n",
      "Epoch 98/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0232 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02241\n",
      "Epoch 99/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0154 - acc: 0.9943 - val_loss: 0.0231 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02241\n",
      "Epoch 100/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0221 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.02241 to 0.02212, saving model to best_model_whistle.hdf5\n",
      "Epoch 101/200\n",
      "12012/12012 [==============================] - 1s 58us/step - loss: 0.0141 - acc: 0.9958 - val_loss: 0.0217 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.02212 to 0.02165, saving model to best_model_whistle.hdf5\n",
      "Epoch 102/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0147 - acc: 0.9951 - val_loss: 0.0224 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.02165\n",
      "Epoch 103/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0138 - acc: 0.9953 - val_loss: 0.0216 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02165 to 0.02158, saving model to best_model_whistle.hdf5\n",
      "Epoch 104/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0273 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.02158\n",
      "Epoch 105/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0146 - acc: 0.9955 - val_loss: 0.0213 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.02158 to 0.02134, saving model to best_model_whistle.hdf5\n",
      "Epoch 106/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0131 - acc: 0.9955 - val_loss: 0.0210 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02134 to 0.02098, saving model to best_model_whistle.hdf5\n",
      "Epoch 107/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.0204 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.02098 to 0.02041, saving model to best_model_whistle.hdf5\n",
      "Epoch 108/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 0.0214 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.02041\n",
      "Epoch 109/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0215 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.02041\n",
      "Epoch 110/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0210 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.02041\n",
      "Epoch 111/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.0206 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.02041\n",
      "Epoch 112/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0200 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02041 to 0.02003, saving model to best_model_whistle.hdf5\n",
      "Epoch 113/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0255 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02003\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0209 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.02003\n",
      "Epoch 115/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0111 - acc: 0.9966 - val_loss: 0.0254 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.02003\n",
      "Epoch 116/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0262 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02003\n",
      "Epoch 117/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0195 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.02003 to 0.01949, saving model to best_model_whistle.hdf5\n",
      "Epoch 118/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0192 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.01949 to 0.01917, saving model to best_model_whistle.hdf5\n",
      "Epoch 119/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0235 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.01917\n",
      "Epoch 120/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0192 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.01917 to 0.01917, saving model to best_model_whistle.hdf5\n",
      "Epoch 121/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0108 - acc: 0.9971 - val_loss: 0.0187 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.01917 to 0.01865, saving model to best_model_whistle.hdf5\n",
      "Epoch 122/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0188 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.01865\n",
      "Epoch 123/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.01865\n",
      "Epoch 124/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0109 - acc: 0.9968 - val_loss: 0.0181 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.01865 to 0.01810, saving model to best_model_whistle.hdf5\n",
      "Epoch 125/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0110 - acc: 0.9961 - val_loss: 0.0214 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.01810\n",
      "Epoch 126/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0194 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.01810\n",
      "Epoch 127/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0182 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.01810\n",
      "Epoch 128/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0099 - acc: 0.9974 - val_loss: 0.0190 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01810\n",
      "Epoch 129/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0191 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01810\n",
      "Epoch 130/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0182 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01810\n",
      "Epoch 131/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0099 - acc: 0.9977 - val_loss: 0.0174 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.01810 to 0.01741, saving model to best_model_whistle.hdf5\n",
      "Epoch 132/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0189 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01741\n",
      "Epoch 133/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0181 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01741\n",
      "Epoch 134/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.0196 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01741\n",
      "Epoch 135/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0172 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.01741 to 0.01721, saving model to best_model_whistle.hdf5\n",
      "Epoch 136/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0089 - acc: 0.9977 - val_loss: 0.0186 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01721\n",
      "Epoch 137/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.01721 to 0.01654, saving model to best_model_whistle.hdf5\n",
      "Epoch 138/200\n",
      "12012/12012 [==============================] - 1s 57us/step - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0175 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01654\n",
      "Epoch 139/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0167 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.01654\n",
      "Epoch 140/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0187 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01654\n",
      "Epoch 141/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0185 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.01654\n",
      "Epoch 142/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0085 - acc: 0.9981 - val_loss: 0.0191 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01654\n",
      "Epoch 143/200\n",
      "12012/12012 [==============================] - 1s 57us/step - loss: 0.0084 - acc: 0.9979 - val_loss: 0.0160 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.01654 to 0.01595, saving model to best_model_whistle.hdf5\n",
      "Epoch 144/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0081 - acc: 0.9981 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01595\n",
      "Epoch 145/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0083 - acc: 0.9979 - val_loss: 0.0177 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01595\n",
      "Epoch 146/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0077 - acc: 0.9980 - val_loss: 0.0158 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.01595 to 0.01584, saving model to best_model_whistle.hdf5\n",
      "Epoch 147/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0076 - acc: 0.9981 - val_loss: 0.0164 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01584\n",
      "Epoch 148/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.0155 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.01584 to 0.01547, saving model to best_model_whistle.hdf5\n",
      "Epoch 149/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0157 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01547\n",
      "Epoch 150/200\n",
      "12012/12012 [==============================] - 1s 58us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0159 - val_acc: 0.9943A: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.9\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01547\n",
      "Epoch 151/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0165 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01547\n",
      "Epoch 152/200\n",
      "12012/12012 [==============================] - 1s 56us/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0159 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01547\n",
      "Epoch 153/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.0150 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.01547 to 0.01501, saving model to best_model_whistle.hdf5\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.0179 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.01501\n",
      "Epoch 155/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0071 - acc: 0.9986 - val_loss: 0.0164 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01501\n",
      "Epoch 156/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0159 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01501\n",
      "Epoch 157/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0156 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.01501\n",
      "Epoch 158/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0153 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01501\n",
      "Epoch 159/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0158 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01501\n",
      "Epoch 160/200\n",
      "12012/12012 [==============================] - 1s 58us/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.0141 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.01501 to 0.01411, saving model to best_model_whistle.hdf5\n",
      "Epoch 161/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0145 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.01411\n",
      "Epoch 162/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0224 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.01411\n",
      "Epoch 163/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01411\n",
      "Epoch 164/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0147 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.01411\n",
      "Epoch 165/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 0.0153 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.01411\n",
      "Epoch 166/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0190 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01411\n",
      "Epoch 167/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0064 - acc: 0.9984 - val_loss: 0.0141 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.01411 to 0.01408, saving model to best_model_whistle.hdf5\n",
      "Epoch 168/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0208 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.01408\n",
      "Epoch 169/200\n",
      "12012/12012 [==============================] - 1s 58us/step - loss: 0.0059 - acc: 0.9992 - val_loss: 0.0150 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01408\n",
      "Epoch 170/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.01408\n",
      "Epoch 171/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0055 - acc: 0.9991 - val_loss: 0.0139 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.01408 to 0.01389, saving model to best_model_whistle.hdf5\n",
      "Epoch 172/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.0149 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01389\n",
      "Epoch 173/200\n",
      "12012/12012 [==============================] - 1s 48us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0142 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.01389\n",
      "Epoch 174/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0137 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.01389 to 0.01366, saving model to best_model_whistle.hdf5\n",
      "Epoch 175/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.01366\n",
      "Epoch 176/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.0136 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.01366 to 0.01355, saving model to best_model_whistle.hdf5\n",
      "Epoch 177/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0248 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01355\n",
      "Epoch 178/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0139 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01355\n",
      "Epoch 179/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0137 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.01355\n",
      "Epoch 180/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.0146 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01355\n",
      "Epoch 181/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0054 - acc: 0.9992 - val_loss: 0.0344 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01355\n",
      "Epoch 182/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 0.0143 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.01355\n",
      "Epoch 183/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.0138 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01355\n",
      "Epoch 184/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0044 - acc: 0.9993 - val_loss: 0.0200 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01355\n",
      "Epoch 185/200\n",
      "12012/12012 [==============================] - 1s 54us/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.0150 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01355\n",
      "Epoch 186/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.0156 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01355\n",
      "Epoch 187/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0134 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.01355 to 0.01340, saving model to best_model_whistle.hdf5\n",
      "Epoch 188/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.0129 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.01340 to 0.01294, saving model to best_model_whistle.hdf5\n",
      "Epoch 189/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.0146 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01294\n",
      "Epoch 190/200\n",
      "12012/12012 [==============================] - 1s 49us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.0182 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01294\n",
      "Epoch 191/200\n",
      "12012/12012 [==============================] - 1s 60us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.0155 - val_acc: 0.9937 0s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01294\n",
      "Epoch 192/200\n",
      "12012/12012 [==============================] - 1s 50us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.0140 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01294\n",
      "Epoch 193/200\n",
      "12012/12012 [==============================] - 1s 51us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0135 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01294\n",
      "Epoch 194/200\n",
      "12012/12012 [==============================] - 1s 58us/step - loss: 0.0039 - acc: 0.9996 - val_loss: 0.0155 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.01294\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.0137 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.01294\n",
      "Epoch 196/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0039 - acc: 0.9996 - val_loss: 0.0238 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01294\n",
      "Epoch 197/200\n",
      "12012/12012 [==============================] - 1s 53us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0125 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.01294 to 0.01254, saving model to best_model_whistle.hdf5\n",
      "Epoch 198/200\n",
      "12012/12012 [==============================] - 1s 55us/step - loss: 0.0036 - acc: 0.9998 - val_loss: 0.0131 - val_acc: 0.99530s - loss: 0.0036 - acc: 0.999\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01254\n",
      "Epoch 199/200\n",
      "12012/12012 [==============================] - 1s 58us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 0.0128 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01254\n",
      "Epoch 200/200\n",
      "12012/12012 [==============================] - 1s 52us/step - loss: 0.0036 - acc: 0.9995 - val_loss: 0.0147 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01254\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(X_train, X_test, Y_train, Y_test, version, feature_type, denoise, plot=False):\n",
    "    \n",
    "    # model training\n",
    "    model, model_checkpoint = two_layer_integrated(X_train)\n",
    "    history = model.fit(X_train, Y_train ,epochs=200, callbacks=[model_checkpoint], batch_size=32, \n",
    "                        validation_data=(X_test, Y_test))\n",
    "\n",
    "    # load the best model weights\n",
    "    model.load_weights('best_model_whistle.hdf5')\n",
    "    \n",
    "    # save model\n",
    "    model.save(\"model_v\" + str(version) + \"_\" + feature_type + \"_\" + str(denoise) + \".h5\")\n",
    "\n",
    "    # summarize history for loss\n",
    "    if plot:\n",
    "        plt.plot(history.history[\"acc\"], c=\"#181818\")\n",
    "        plt.plot(history.history[\"val_acc\"], c=\"#1ED760\")\n",
    "        plt.title(\"Model accuracy V\" + str(version) + \"_\" + feature_type + \"_\" + str(denoise))\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.legend([\"Train\", \"Val\"], loc=\"lower right\")\n",
    "        plt.ylim([0.83,1.01])\n",
    "        plt.show()\n",
    "        \n",
    "    return model\n",
    "\n",
    "def train_model(version, feature_type, denoise):\n",
    "    # load data\n",
    "    mass_data = build_features(version, feature_type, denoise)\n",
    "    \n",
    "    # train test split\n",
    "    X, Y = feature_target_split(mass_data)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, \n",
    "                                                        random_state=12, shuffle=True)\n",
    "    \n",
    "    return X_test, Y_test, train(X_train, X_test, Y_train, Y_test, version, feature_type, denoise, plot=True)\n",
    "        \n",
    "\n",
    "X_test, Y_test, model = train_model(1, \"fft\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9888551235198975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x14c505be860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_test, [1 if prediction > .5 else 0 for prediction in model.predict(X_test)[:,0]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[False, True])\n",
    "print(\"acc:\", model.evaluate(X_test, Y_test, verbose=0)[1])\n",
    "disp.plot(cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe succesfully loaded from csv!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pca_decompose(version, feature_type, denoise):\n",
    "    # load data\n",
    "    df = build_features(version, feature_type, denoise)\n",
    "    \n",
    "    # pca decomposition\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(df.loc[:, df.columns != \"target\"])\n",
    "    pca_df = pd.DataFrame(data = principalComponents, columns = [\"dim_1\", \"dim_2\"])\n",
    "    pca_df.insert (0, \"target\", df[\"target\"])\n",
    "    \n",
    "    # plot\n",
    "    colors = [\"#181818\", \"#1ED760\"]\n",
    "    for i, label in enumerate(pca_df[\"target\"].unique()):\n",
    "        pca_label = pca_df[pca_df[\"target\"] == label]\n",
    "        plt.scatter(pca_label[\"dim_1\"], pca_label[\"dim_2\"], label = bool(label), alpha=0.3, c = colors[i])\n",
    "    plt.title(\"PCA decomposition V\" + str(version) + \"_\" + feature_type + \"_\" + str(denoise))\n",
    "    plt.xlim([-1,1.5])\n",
    "    plt.ylim([-1,1.5])\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "pca_decompose(2, \"mfcc\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7527973e53ae4881ad7a680deaade798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=195.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=195.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=195.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=95.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=95.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=95.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=84.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=84.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=84.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=68.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=68.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=68.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=70.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=70.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=70.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=54.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=54.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=54.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=184.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=184.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=184.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=68.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=68.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=68.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=89.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=89.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=89.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=95.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=95.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=95.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=93.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=93.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=93.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=131.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=131.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=131.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=183.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=183.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=183.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=123.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=123.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=123.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=108.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=108.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=108.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=81.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhistleTest_tuhhnao16.wav 28116000 no fit ;(\n",
      "\n",
      "Train on 12012 samples, validate on 3003 samples\n",
      "Epoch 1/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.3141 - acc: 0.8895 - val_loss: 0.1928 - val_acc: 0.9227\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19281, saving model to best_model_whistle.hdf5\n",
      "Epoch 2/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.1672 - acc: 0.9391 - val_loss: 0.1463 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19281 to 0.14634, saving model to best_model_whistle.hdf5\n",
      "Epoch 3/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.1346 - acc: 0.9520 - val_loss: 0.1289 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14634 to 0.12890, saving model to best_model_whistle.hdf5\n",
      "Epoch 4/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.1202 - acc: 0.9568 - val_loss: 0.1098 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12890 to 0.10985, saving model to best_model_whistle.hdf5\n",
      "Epoch 5/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.1108 - acc: 0.9594 - val_loss: 0.1032 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10985 to 0.10317, saving model to best_model_whistle.hdf5\n",
      "Epoch 6/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.1051 - acc: 0.9619 - val_loss: 0.0976 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10317 to 0.09759, saving model to best_model_whistle.hdf5\n",
      "Epoch 7/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1001 - acc: 0.9629 - val_loss: 0.0928 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09759 to 0.09276, saving model to best_model_whistle.hdf5\n",
      "Epoch 8/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0946 - acc: 0.9654 - val_loss: 0.0880 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09276 to 0.08805, saving model to best_model_whistle.hdf5\n",
      "Epoch 9/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0901 - acc: 0.9669 - val_loss: 0.0871 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08805 to 0.08710, saving model to best_model_whistle.hdf5\n",
      "Epoch 10/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0861 - acc: 0.9682 - val_loss: 0.0811 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08710 to 0.08109, saving model to best_model_whistle.hdf5\n",
      "Epoch 11/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0832 - acc: 0.9689 - val_loss: 0.0796 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08109 to 0.07960, saving model to best_model_whistle.hdf5\n",
      "Epoch 12/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0798 - acc: 0.9700 - val_loss: 0.0755 - val_acc: 0.9710\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07960 to 0.07550, saving model to best_model_whistle.hdf5\n",
      "Epoch 13/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0768 - acc: 0.9711 - val_loss: 0.0749 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.07550 to 0.07485, saving model to best_model_whistle.hdf5\n",
      "Epoch 14/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0735 - acc: 0.9731 - val_loss: 0.0737 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07485 to 0.07369, saving model to best_model_whistle.hdf5\n",
      "Epoch 15/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0709 - acc: 0.9735 - val_loss: 0.0680 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07369 to 0.06804, saving model to best_model_whistle.hdf5\n",
      "Epoch 16/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0688 - acc: 0.9744 - val_loss: 0.0732 - val_acc: 0.9710\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06804\n",
      "Epoch 17/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0678 - acc: 0.9744 - val_loss: 0.0665 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06804 to 0.06650, saving model to best_model_whistle.hdf5\n",
      "Epoch 18/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0638 - acc: 0.9753 - val_loss: 0.0660 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06650 to 0.06597, saving model to best_model_whistle.hdf5\n",
      "Epoch 19/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0624 - acc: 0.9769 - val_loss: 0.0616 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06597 to 0.06163, saving model to best_model_whistle.hdf5\n",
      "Epoch 20/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0617 - acc: 0.9764 - val_loss: 0.0597 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06163 to 0.05974, saving model to best_model_whistle.hdf5\n",
      "Epoch 21/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0583 - acc: 0.9789 - val_loss: 0.0624 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05974\n",
      "Epoch 22/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0573 - acc: 0.9785 - val_loss: 0.0624 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05974\n",
      "Epoch 23/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0558 - acc: 0.9789 - val_loss: 0.0556 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05974 to 0.05563, saving model to best_model_whistle.hdf5\n",
      "Epoch 24/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0545 - acc: 0.9806 - val_loss: 0.0563 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.05563\n",
      "Epoch 25/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0531 - acc: 0.9803 - val_loss: 0.0539 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.05563 to 0.05386, saving model to best_model_whistle.hdf5\n",
      "Epoch 26/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0533 - acc: 0.9808 - val_loss: 0.0579 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.05386\n",
      "Epoch 27/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0512 - acc: 0.9814 - val_loss: 0.0553 - val_acc: 0.9767\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05386\n",
      "Epoch 28/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0491 - acc: 0.9822 - val_loss: 0.0523 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05386 to 0.05233, saving model to best_model_whistle.hdf5\n",
      "Epoch 29/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0482 - acc: 0.9813 - val_loss: 0.0541 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.05233\n",
      "Epoch 30/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0476 - acc: 0.9820 - val_loss: 0.0616 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.05233\n",
      "Epoch 31/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0461 - acc: 0.9818 - val_loss: 0.0479 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05233 to 0.04791, saving model to best_model_whistle.hdf5\n",
      "Epoch 32/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0455 - acc: 0.9832 - val_loss: 0.0470 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04791 to 0.04695, saving model to best_model_whistle.hdf5\n",
      "Epoch 33/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0444 - acc: 0.9834 - val_loss: 0.0472 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04695\n",
      "Epoch 34/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0431 - acc: 0.9830 - val_loss: 0.0463 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04695 to 0.04633, saving model to best_model_whistle.hdf5\n",
      "Epoch 35/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0422 - acc: 0.9848 - val_loss: 0.0452 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04633 to 0.04516, saving model to best_model_whistle.hdf5\n",
      "Epoch 36/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0423 - acc: 0.9841 - val_loss: 0.0444 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04516 to 0.04441, saving model to best_model_whistle.hdf5\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0413 - acc: 0.9848 - val_loss: 0.0434 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04441 to 0.04345, saving model to best_model_whistle.hdf5\n",
      "Epoch 38/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0403 - acc: 0.9853 - val_loss: 0.0442 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04345\n",
      "Epoch 39/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0397 - acc: 0.9855 - val_loss: 0.0455 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.04345\n",
      "Epoch 40/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0385 - acc: 0.9870 - val_loss: 0.0416 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.04345 to 0.04158, saving model to best_model_whistle.hdf5\n",
      "Epoch 41/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0389 - acc: 0.9853 - val_loss: 0.0409 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04158 to 0.04095, saving model to best_model_whistle.hdf5\n",
      "Epoch 42/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0374 - acc: 0.9858 - val_loss: 0.0475 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.04095\n",
      "Epoch 43/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0368 - acc: 0.9864 - val_loss: 0.0418 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.04095\n",
      "Epoch 44/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0365 - acc: 0.9859 - val_loss: 0.0422 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.04095\n",
      "Epoch 45/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0352 - acc: 0.9868 - val_loss: 0.0399 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.04095 to 0.03992, saving model to best_model_whistle.hdf5\n",
      "Epoch 46/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0343 - acc: 0.9870 - val_loss: 0.0385 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03992 to 0.03855, saving model to best_model_whistle.hdf5\n",
      "Epoch 47/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0346 - acc: 0.9873 - val_loss: 0.0397 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03855\n",
      "Epoch 48/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0341 - acc: 0.9874 - val_loss: 0.0384 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03855 to 0.03841, saving model to best_model_whistle.hdf5\n",
      "Epoch 49/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0333 - acc: 0.9880 - val_loss: 0.0378 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03841 to 0.03775, saving model to best_model_whistle.hdf5\n",
      "Epoch 50/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0325 - acc: 0.9878 - val_loss: 0.0378 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03775\n",
      "Epoch 51/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0320 - acc: 0.9880 - val_loss: 0.0373 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03775 to 0.03734, saving model to best_model_whistle.hdf5\n",
      "Epoch 52/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0324 - acc: 0.9873 - val_loss: 0.0381 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03734\n",
      "Epoch 53/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0311 - acc: 0.9879 - val_loss: 0.0359 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03734 to 0.03586, saving model to best_model_whistle.hdf5\n",
      "Epoch 54/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0308 - acc: 0.9883 - val_loss: 0.0357 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03586 to 0.03571, saving model to best_model_whistle.hdf5\n",
      "Epoch 55/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0303 - acc: 0.9888 - val_loss: 0.0386 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03571\n",
      "Epoch 56/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0310 - acc: 0.9882 - val_loss: 0.0377 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03571\n",
      "Epoch 57/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0310 - acc: 0.9882 - val_loss: 0.0341 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.03571 to 0.03411, saving model to best_model_whistle.hdf5\n",
      "Epoch 58/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0293 - acc: 0.9895 - val_loss: 0.0350 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03411\n",
      "Epoch 59/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0289 - acc: 0.9888 - val_loss: 0.0336 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03411 to 0.03364, saving model to best_model_whistle.hdf5\n",
      "Epoch 60/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0288 - acc: 0.9897 - val_loss: 0.0341 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03364\n",
      "Epoch 61/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0280 - acc: 0.9898 - val_loss: 0.0353 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03364\n",
      "Epoch 62/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0283 - acc: 0.9892 - val_loss: 0.0376 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03364\n",
      "Epoch 63/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0278 - acc: 0.9895 - val_loss: 0.0336 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.03364 to 0.03360, saving model to best_model_whistle.hdf5\n",
      "Epoch 64/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0272 - acc: 0.9905 - val_loss: 0.0352 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03360\n",
      "Epoch 65/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0270 - acc: 0.9896 - val_loss: 0.0329 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.03360 to 0.03285, saving model to best_model_whistle.hdf5\n",
      "Epoch 66/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0265 - acc: 0.9900 - val_loss: 0.0337 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03285\n",
      "Epoch 67/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0257 - acc: 0.9903 - val_loss: 0.0318 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03285 to 0.03182, saving model to best_model_whistle.hdf5\n",
      "Epoch 68/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0256 - acc: 0.9908 - val_loss: 0.0325 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03182\n",
      "Epoch 69/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0253 - acc: 0.9909 - val_loss: 0.0314 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.03182 to 0.03145, saving model to best_model_whistle.hdf5\n",
      "Epoch 70/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0246 - acc: 0.9914 - val_loss: 0.0318 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03145\n",
      "Epoch 71/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0250 - acc: 0.9907 - val_loss: 0.0316 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03145\n",
      "Epoch 72/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0243 - acc: 0.9914 - val_loss: 0.0347 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03145\n",
      "Epoch 73/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0252 - acc: 0.9908 - val_loss: 0.0377 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03145\n",
      "Epoch 74/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0241 - acc: 0.9906 - val_loss: 0.0303 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.03145 to 0.03030, saving model to best_model_whistle.hdf5\n",
      "Epoch 75/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0233 - acc: 0.9913 - val_loss: 0.0307 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03030\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0231 - acc: 0.9912 - val_loss: 0.0322 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03030\n",
      "Epoch 77/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0228 - acc: 0.9918 - val_loss: 0.0296 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.03030 to 0.02959, saving model to best_model_whistle.hdf5\n",
      "Epoch 78/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0227 - acc: 0.9917 - val_loss: 0.0305 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02959\n",
      "Epoch 79/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0222 - acc: 0.9920 - val_loss: 0.0303 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02959\n",
      "Epoch 80/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0218 - acc: 0.9918 - val_loss: 0.0310 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02959\n",
      "Epoch 81/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0220 - acc: 0.9919 - val_loss: 0.0308 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02959\n",
      "Epoch 82/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0216 - acc: 0.9919 - val_loss: 0.0293 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.02959 to 0.02927, saving model to best_model_whistle.hdf5\n",
      "Epoch 83/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0286 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.02927 to 0.02859, saving model to best_model_whistle.hdf5\n",
      "Epoch 84/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0203 - acc: 0.9930 - val_loss: 0.0286 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02859\n",
      "Epoch 85/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0205 - acc: 0.9925 - val_loss: 0.0296 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02859\n",
      "Epoch 86/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0205 - acc: 0.9925 - val_loss: 0.0281 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.02859 to 0.02813, saving model to best_model_whistle.hdf5\n",
      "Epoch 87/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0198 - acc: 0.9926 - val_loss: 0.0297 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02813\n",
      "Epoch 88/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0197 - acc: 0.9933 - val_loss: 0.0279 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.02813 to 0.02793, saving model to best_model_whistle.hdf5\n",
      "Epoch 89/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0201 - acc: 0.9928 - val_loss: 0.0278 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.02793 to 0.02781, saving model to best_model_whistle.hdf5\n",
      "Epoch 90/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0274 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02781 to 0.02744, saving model to best_model_whistle.hdf5\n",
      "Epoch 91/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0193 - acc: 0.9929 - val_loss: 0.0301 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02744\n",
      "Epoch 92/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0191 - acc: 0.9925 - val_loss: 0.0284 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02744\n",
      "Epoch 93/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0287 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02744\n",
      "Epoch 94/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0186 - acc: 0.9937 - val_loss: 0.0305 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02744\n",
      "Epoch 95/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0181 - acc: 0.9940 - val_loss: 0.0270 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.02744 to 0.02703, saving model to best_model_whistle.hdf5\n",
      "Epoch 96/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0178 - acc: 0.9938 - val_loss: 0.0286 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02703\n",
      "Epoch 97/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0180 - acc: 0.9939 - val_loss: 0.0317 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02703\n",
      "Epoch 98/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0179 - acc: 0.9938 - val_loss: 0.0265 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02703 to 0.02651, saving model to best_model_whistle.hdf5\n",
      "Epoch 99/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0292 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02651\n",
      "Epoch 100/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0164 - acc: 0.9955 - val_loss: 0.0262 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.02651 to 0.02625, saving model to best_model_whistle.hdf5\n",
      "Epoch 101/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0182 - acc: 0.9936 - val_loss: 0.0268 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.02625\n",
      "Epoch 102/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0261 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02625 to 0.02612, saving model to best_model_whistle.hdf5\n",
      "Epoch 103/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0173 - acc: 0.9943 - val_loss: 0.0266 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.02612\n",
      "Epoch 104/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0256 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.02612 to 0.02559, saving model to best_model_whistle.hdf5\n",
      "Epoch 105/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0262 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02559\n",
      "Epoch 106/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0160 - acc: 0.9951 - val_loss: 0.0308 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.02559\n",
      "Epoch 107/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0158 - acc: 0.9948 - val_loss: 0.0266 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.02559\n",
      "Epoch 108/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0157 - acc: 0.9948 - val_loss: 0.0252 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.02559 to 0.02520, saving model to best_model_whistle.hdf5\n",
      "Epoch 109/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.0259 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.02520\n",
      "Epoch 110/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0275 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.02520\n",
      "Epoch 111/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0149 - acc: 0.9958 - val_loss: 0.0272 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.02520\n",
      "Epoch 112/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0145 - acc: 0.9950 - val_loss: 0.0251 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02520 to 0.02506, saving model to best_model_whistle.hdf5\n",
      "Epoch 113/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0171 - acc: 0.9935 - val_loss: 0.0262 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02506\n",
      "Epoch 114/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0144 - acc: 0.9955 - val_loss: 0.0255 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.02506\n",
      "Epoch 115/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0143 - acc: 0.9957 - val_loss: 0.0246 - val_acc: 0.9907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00115: val_loss improved from 0.02506 to 0.02458, saving model to best_model_whistle.hdf5\n",
      "Epoch 116/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0139 - acc: 0.9957 - val_loss: 0.0246 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02458\n",
      "Epoch 117/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0140 - acc: 0.9958 - val_loss: 0.0249 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.02458\n",
      "Epoch 118/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0257 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.02458\n",
      "Epoch 119/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0257 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.02458\n",
      "Epoch 120/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0130 - acc: 0.9959 - val_loss: 0.0246 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.02458\n",
      "Epoch 121/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0250 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.02458\n",
      "Epoch 122/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0269 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.02458\n",
      "Epoch 123/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0278 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.02458\n",
      "Epoch 124/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0126 - acc: 0.9968 - val_loss: 0.0245 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.02458 to 0.02453, saving model to best_model_whistle.hdf5\n",
      "Epoch 125/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0241 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.02453 to 0.02406, saving model to best_model_whistle.hdf5\n",
      "Epoch 126/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 0.0246 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.02406\n",
      "Epoch 127/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0239 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02406 to 0.02392, saving model to best_model_whistle.hdf5\n",
      "Epoch 128/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0245 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.02392\n",
      "Epoch 129/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0244 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.02392\n",
      "Epoch 130/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0256 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.02392\n",
      "Epoch 131/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0117 - acc: 0.9966 - val_loss: 0.0245 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.02392\n",
      "Epoch 132/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0119 - acc: 0.9963 - val_loss: 0.0240 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.02392\n",
      "Epoch 133/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.0274 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.02392\n",
      "Epoch 134/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0265 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.02392\n",
      "Epoch 135/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0240 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.02392\n",
      "Epoch 136/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0106 - acc: 0.9972 - val_loss: 0.0248 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.02392\n",
      "Epoch 137/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.0239 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.02392 to 0.02385, saving model to best_model_whistle.hdf5\n",
      "Epoch 138/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.0258 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.02385\n",
      "Epoch 139/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.0249 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.02385\n",
      "Epoch 140/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0105 - acc: 0.9969 - val_loss: 0.0236 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02385 to 0.02365, saving model to best_model_whistle.hdf5\n",
      "Epoch 141/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0241 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.02365\n",
      "Epoch 142/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.0229 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.02365 to 0.02291, saving model to best_model_whistle.hdf5\n",
      "Epoch 143/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0098 - acc: 0.9975 - val_loss: 0.0244 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.02291\n",
      "Epoch 144/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0098 - acc: 0.9972 - val_loss: 0.0241 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.02291\n",
      "Epoch 145/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0099 - acc: 0.9971 - val_loss: 0.0235 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.02291\n",
      "Epoch 146/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0097 - acc: 0.9972 - val_loss: 0.0247 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.02291\n",
      "Epoch 147/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0231 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.02291\n",
      "Epoch 148/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0238 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.02291\n",
      "Epoch 149/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0268 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.02291\n",
      "Epoch 150/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0229 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.02291\n",
      "Epoch 151/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0233 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.02291\n",
      "Epoch 152/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0248 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.02291\n",
      "Epoch 153/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0225 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.02291 to 0.02255, saving model to best_model_whistle.hdf5\n",
      "Epoch 154/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0226 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.02255\n",
      "Epoch 155/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0234 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.02255\n",
      "Epoch 156/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.0220 - val_acc: 0.9917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00156: val_loss improved from 0.02255 to 0.02204, saving model to best_model_whistle.hdf5\n",
      "Epoch 157/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0081 - acc: 0.9984 - val_loss: 0.0233 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.02204\n",
      "Epoch 158/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0247 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.02204\n",
      "Epoch 159/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0221 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.02204\n",
      "Epoch 160/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0259 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.02204\n",
      "Epoch 161/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0253 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.02204\n",
      "Epoch 162/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0077 - acc: 0.9983 - val_loss: 0.0244 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.02204\n",
      "Epoch 163/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0077 - acc: 0.9984 - val_loss: 0.0224 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.02204\n",
      "Epoch 164/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0077 - acc: 0.9983 - val_loss: 0.0223 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.02204\n",
      "Epoch 165/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0226 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.02204\n",
      "Epoch 166/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0221 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.02204\n",
      "Epoch 167/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.0229 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.02204\n",
      "Epoch 168/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0229 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.02204\n",
      "Epoch 169/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0224 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.02204\n",
      "Epoch 170/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0271 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.02204\n",
      "Epoch 171/200\n",
      "12012/12012 [==============================] - 0s 26us/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.0296 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.02204\n",
      "Epoch 172/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0236 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.02204\n",
      "Epoch 173/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0069 - acc: 0.9984 - val_loss: 0.0281 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.02204\n",
      "Epoch 174/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0264 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.02204\n",
      "Epoch 175/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0220 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.02204 to 0.02202, saving model to best_model_whistle.hdf5\n",
      "Epoch 176/200\n",
      "12012/12012 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.998 - 0s 28us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0229 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.02202\n",
      "Epoch 177/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0066 - acc: 0.9989 - val_loss: 0.0263 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.02202\n",
      "Epoch 178/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.02202\n",
      "Epoch 179/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0066 - acc: 0.9985 - val_loss: 0.0216 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.02202 to 0.02158, saving model to best_model_whistle.hdf5\n",
      "Epoch 180/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.0219 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.02158\n",
      "Epoch 181/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0062 - acc: 0.9989 - val_loss: 0.0220 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.02158\n",
      "Epoch 182/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0228 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.02158\n",
      "Epoch 183/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.0217 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.02158\n",
      "Epoch 184/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0060 - acc: 0.9985 - val_loss: 0.0222 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.02158\n",
      "Epoch 185/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0219 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.02158\n",
      "Epoch 186/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0216 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.02158\n",
      "Epoch 187/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0221 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.02158\n",
      "Epoch 188/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0222 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.02158\n",
      "Epoch 189/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0224 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.02158\n",
      "Epoch 190/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.0243 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.02158\n",
      "Epoch 191/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0248 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.02158\n",
      "Epoch 192/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0216 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.02158\n",
      "Epoch 193/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.0250 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.02158\n",
      "Epoch 194/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0220 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.02158\n",
      "Epoch 195/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0049 - acc: 0.9995 - val_loss: 0.0228 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.02158\n",
      "Epoch 196/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.0241 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.02158\n",
      "Epoch 197/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 0.0224 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.02158\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 0.0216 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.02158\n",
      "Epoch 199/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.0215 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.02158 to 0.02149, saving model to best_model_whistle.hdf5\n",
      "Epoch 200/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.0230 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.02149\n",
      "Train on 12012 samples, validate on 3003 samples\n",
      "Epoch 1/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.3099 - acc: 0.8885 - val_loss: 0.2075 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20750, saving model to best_model_whistle.hdf5\n",
      "Epoch 2/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.1631 - acc: 0.9404 - val_loss: 0.1593 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20750 to 0.15935, saving model to best_model_whistle.hdf5\n",
      "Epoch 3/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1327 - acc: 0.9546 - val_loss: 0.1461 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15935 to 0.14607, saving model to best_model_whistle.hdf5\n",
      "Epoch 4/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1197 - acc: 0.9577 - val_loss: 0.1303 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14607 to 0.13027, saving model to best_model_whistle.hdf5\n",
      "Epoch 5/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.1105 - acc: 0.9599 - val_loss: 0.1228 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13027 to 0.12277, saving model to best_model_whistle.hdf5\n",
      "Epoch 6/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.1038 - acc: 0.9624 - val_loss: 0.1182 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12277 to 0.11822, saving model to best_model_whistle.hdf5\n",
      "Epoch 7/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0997 - acc: 0.9635 - val_loss: 0.1128 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11822 to 0.11280, saving model to best_model_whistle.hdf5\n",
      "Epoch 8/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0941 - acc: 0.9655 - val_loss: 0.1099 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11280 to 0.10989, saving model to best_model_whistle.hdf5\n",
      "Epoch 9/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0910 - acc: 0.9650 - val_loss: 0.1032 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10989 to 0.10324, saving model to best_model_whistle.hdf5\n",
      "Epoch 10/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0864 - acc: 0.9671 - val_loss: 0.1013 - val_acc: 0.9620\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10324 to 0.10128, saving model to best_model_whistle.hdf5\n",
      "Epoch 11/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0836 - acc: 0.9685 - val_loss: 0.0958 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10128 to 0.09578, saving model to best_model_whistle.hdf5\n",
      "Epoch 12/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0797 - acc: 0.9694 - val_loss: 0.0935 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09578 to 0.09349, saving model to best_model_whistle.hdf5\n",
      "Epoch 13/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0762 - acc: 0.9716 - val_loss: 0.0877 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09349 to 0.08767, saving model to best_model_whistle.hdf5\n",
      "Epoch 14/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0732 - acc: 0.9721 - val_loss: 0.0972 - val_acc: 0.9620\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08767\n",
      "Epoch 15/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0706 - acc: 0.9723 - val_loss: 0.0813 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08767 to 0.08131, saving model to best_model_whistle.hdf5\n",
      "Epoch 16/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0685 - acc: 0.9739 - val_loss: 0.0785 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08131 to 0.07847, saving model to best_model_whistle.hdf5\n",
      "Epoch 17/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0646 - acc: 0.9752 - val_loss: 0.0740 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07847 to 0.07403, saving model to best_model_whistle.hdf5\n",
      "Epoch 18/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0629 - acc: 0.9761 - val_loss: 0.0752 - val_acc: 0.9710\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.07403\n",
      "Epoch 19/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0609 - acc: 0.9778 - val_loss: 0.0763 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07403\n",
      "Epoch 20/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0582 - acc: 0.9788 - val_loss: 0.0709 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07403 to 0.07094, saving model to best_model_whistle.hdf5\n",
      "Epoch 21/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0554 - acc: 0.9797 - val_loss: 0.0661 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.07094 to 0.06607, saving model to best_model_whistle.hdf5\n",
      "Epoch 22/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0548 - acc: 0.9794 - val_loss: 0.0744 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06607\n",
      "Epoch 23/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0530 - acc: 0.9796 - val_loss: 0.0629 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06607 to 0.06287, saving model to best_model_whistle.hdf5\n",
      "Epoch 24/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0511 - acc: 0.9804 - val_loss: 0.0606 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06287 to 0.06062, saving model to best_model_whistle.hdf5\n",
      "Epoch 25/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0490 - acc: 0.9819 - val_loss: 0.0594 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06062 to 0.05941, saving model to best_model_whistle.hdf5\n",
      "Epoch 26/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0481 - acc: 0.9824 - val_loss: 0.0567 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05941 to 0.05670, saving model to best_model_whistle.hdf5\n",
      "Epoch 27/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0469 - acc: 0.9820 - val_loss: 0.0577 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05670\n",
      "Epoch 28/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0448 - acc: 0.9833 - val_loss: 0.0568 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05670\n",
      "Epoch 29/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0436 - acc: 0.9840 - val_loss: 0.0550 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05670 to 0.05505, saving model to best_model_whistle.hdf5\n",
      "Epoch 30/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0439 - acc: 0.9839 - val_loss: 0.0536 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05505 to 0.05356, saving model to best_model_whistle.hdf5\n",
      "Epoch 31/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0415 - acc: 0.9846 - val_loss: 0.0553 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05356\n",
      "Epoch 32/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0407 - acc: 0.9844 - val_loss: 0.0575 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.05356\n",
      "Epoch 33/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0401 - acc: 0.9853 - val_loss: 0.0494 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.05356 to 0.04935, saving model to best_model_whistle.hdf5\n",
      "Epoch 34/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0386 - acc: 0.9858 - val_loss: 0.0494 - val_acc: 0.9830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04935\n",
      "Epoch 35/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0382 - acc: 0.9860 - val_loss: 0.0487 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04935 to 0.04867, saving model to best_model_whistle.hdf5\n",
      "Epoch 36/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0364 - acc: 0.9870 - val_loss: 0.0479 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04867 to 0.04792, saving model to best_model_whistle.hdf5\n",
      "Epoch 37/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0357 - acc: 0.9873 - val_loss: 0.0466 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04792 to 0.04658, saving model to best_model_whistle.hdf5\n",
      "Epoch 38/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0348 - acc: 0.9871 - val_loss: 0.0465 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04658 to 0.04652, saving model to best_model_whistle.hdf5\n",
      "Epoch 39/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0344 - acc: 0.9873 - val_loss: 0.0491 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.04652\n",
      "Epoch 40/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0340 - acc: 0.9873 - val_loss: 0.0439 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.04652 to 0.04389, saving model to best_model_whistle.hdf5\n",
      "Epoch 41/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0327 - acc: 0.9879 - val_loss: 0.0482 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.04389\n",
      "Epoch 42/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0316 - acc: 0.9883 - val_loss: 0.0449 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.04389\n",
      "Epoch 43/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0319 - acc: 0.9882 - val_loss: 0.0422 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04389 to 0.04223, saving model to best_model_whistle.hdf5\n",
      "Epoch 44/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0304 - acc: 0.9885 - val_loss: 0.0419 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04223 to 0.04187, saving model to best_model_whistle.hdf5\n",
      "Epoch 45/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0307 - acc: 0.9887 - val_loss: 0.0451 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.04187\n",
      "Epoch 46/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0301 - acc: 0.9883 - val_loss: 0.0401 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.04187 to 0.04012, saving model to best_model_whistle.hdf5\n",
      "Epoch 47/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0289 - acc: 0.9895 - val_loss: 0.0437 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.04012\n",
      "Epoch 48/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0281 - acc: 0.9893 - val_loss: 0.0407 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.04012\n",
      "Epoch 49/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0276 - acc: 0.9898 - val_loss: 0.0439 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.04012\n",
      "Epoch 50/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0272 - acc: 0.9907 - val_loss: 0.0403 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04012\n",
      "Epoch 51/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0272 - acc: 0.9898 - val_loss: 0.0573 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04012\n",
      "Epoch 52/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0268 - acc: 0.9903 - val_loss: 0.0417 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.04012\n",
      "Epoch 53/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0263 - acc: 0.9905 - val_loss: 0.0371 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.04012 to 0.03707, saving model to best_model_whistle.hdf5\n",
      "Epoch 54/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0260 - acc: 0.9898 - val_loss: 0.0369 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03707 to 0.03690, saving model to best_model_whistle.hdf5\n",
      "Epoch 55/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0251 - acc: 0.9911 - val_loss: 0.0374 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03690\n",
      "Epoch 56/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0247 - acc: 0.9908 - val_loss: 0.0373 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03690\n",
      "Epoch 57/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0247 - acc: 0.9908 - val_loss: 0.0365 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.03690 to 0.03653, saving model to best_model_whistle.hdf5\n",
      "Epoch 58/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0239 - acc: 0.9918 - val_loss: 0.0357 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.03653 to 0.03572, saving model to best_model_whistle.hdf5\n",
      "Epoch 59/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0235 - acc: 0.9923 - val_loss: 0.0352 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03572 to 0.03521, saving model to best_model_whistle.hdf5\n",
      "Epoch 60/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0236 - acc: 0.9912 - val_loss: 0.0356 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03521\n",
      "Epoch 61/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0228 - acc: 0.9922 - val_loss: 0.0349 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03521 to 0.03486, saving model to best_model_whistle.hdf5\n",
      "Epoch 62/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0224 - acc: 0.9923 - val_loss: 0.0359 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03486\n",
      "Epoch 63/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0222 - acc: 0.9918 - val_loss: 0.0345 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.03486 to 0.03453, saving model to best_model_whistle.hdf5\n",
      "Epoch 64/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0223 - acc: 0.9924 - val_loss: 0.0391 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03453\n",
      "Epoch 65/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0347 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03453\n",
      "Epoch 66/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0207 - acc: 0.9928 - val_loss: 0.0334 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.03453 to 0.03340, saving model to best_model_whistle.hdf5\n",
      "Epoch 67/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0224 - acc: 0.9921 - val_loss: 0.0394 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03340\n",
      "Epoch 68/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0208 - acc: 0.9930 - val_loss: 0.0367 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03340\n",
      "Epoch 69/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0412 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03340\n",
      "Epoch 70/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0199 - acc: 0.9927 - val_loss: 0.0371 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03340\n",
      "Epoch 71/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0321 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03340 to 0.03212, saving model to best_model_whistle.hdf5\n",
      "Epoch 72/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0326 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03212\n",
      "Epoch 73/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0188 - acc: 0.9941 - val_loss: 0.0323 - val_acc: 0.9890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03212\n",
      "Epoch 74/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0186 - acc: 0.9938 - val_loss: 0.0336 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03212\n",
      "Epoch 75/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0322 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03212\n",
      "Epoch 76/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0182 - acc: 0.9937 - val_loss: 0.0340 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03212\n",
      "Epoch 77/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.0331 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03212\n",
      "Epoch 78/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0187 - acc: 0.9942 - val_loss: 0.0324 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03212\n",
      "Epoch 79/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0313 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.03212 to 0.03129, saving model to best_model_whistle.hdf5\n",
      "Epoch 80/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0169 - acc: 0.9948 - val_loss: 0.0372 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03129\n",
      "Epoch 81/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0307 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.03129 to 0.03066, saving model to best_model_whistle.hdf5\n",
      "Epoch 82/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0307 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03066\n",
      "Epoch 83/200\n",
      "12012/12012 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.995 - 0s 32us/step - loss: 0.0160 - acc: 0.9947 - val_loss: 0.0310 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03066\n",
      "Epoch 84/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.0289 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.03066 to 0.02889, saving model to best_model_whistle.hdf5\n",
      "Epoch 85/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0159 - acc: 0.9952 - val_loss: 0.0312 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02889\n",
      "Epoch 86/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0163 - acc: 0.9953 - val_loss: 0.0294 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02889\n",
      "Epoch 87/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0157 - acc: 0.9956 - val_loss: 0.0298 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02889\n",
      "Epoch 88/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0151 - acc: 0.9958 - val_loss: 0.0299 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02889\n",
      "Epoch 89/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0145 - acc: 0.9955 - val_loss: 0.0351 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02889\n",
      "Epoch 90/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0281 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02889 to 0.02808, saving model to best_model_whistle.hdf5\n",
      "Epoch 91/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0146 - acc: 0.9958 - val_loss: 0.0304 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02808\n",
      "Epoch 92/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0279 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02808 to 0.02785, saving model to best_model_whistle.hdf5\n",
      "Epoch 93/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0143 - acc: 0.9960 - val_loss: 0.0286 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02785\n",
      "Epoch 94/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0143 - acc: 0.9957 - val_loss: 0.0279 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02785\n",
      "Epoch 95/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0318 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02785\n",
      "Epoch 96/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0137 - acc: 0.9961 - val_loss: 0.0286 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02785\n",
      "Epoch 97/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0144 - acc: 0.9957 - val_loss: 0.0271 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02785 to 0.02708, saving model to best_model_whistle.hdf5\n",
      "Epoch 98/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0140 - acc: 0.9957 - val_loss: 0.0258 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02708 to 0.02579, saving model to best_model_whistle.hdf5\n",
      "Epoch 99/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0132 - acc: 0.9966 - val_loss: 0.0264 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02579\n",
      "Epoch 100/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0255 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.02579 to 0.02546, saving model to best_model_whistle.hdf5\n",
      "Epoch 101/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0263 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.02546\n",
      "Epoch 102/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0130 - acc: 0.9963 - val_loss: 0.0254 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02546 to 0.02541, saving model to best_model_whistle.hdf5\n",
      "Epoch 103/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0125 - acc: 0.9963 - val_loss: 0.0270 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.02541\n",
      "Epoch 104/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0127 - acc: 0.9963 - val_loss: 0.0270 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.02541\n",
      "Epoch 105/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0265 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02541\n",
      "Epoch 106/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0128 - acc: 0.9962 - val_loss: 0.0273 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.02541\n",
      "Epoch 107/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0246 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.02541 to 0.02463, saving model to best_model_whistle.hdf5\n",
      "Epoch 108/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0119 - acc: 0.9967 - val_loss: 0.0252 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.02463\n",
      "Epoch 109/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0118 - acc: 0.9968 - val_loss: 0.0263 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.02463\n",
      "Epoch 110/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 0.0329 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.02463\n",
      "Epoch 111/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0121 - acc: 0.9967 - val_loss: 0.0261 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.02463\n",
      "Epoch 112/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0113 - acc: 0.9969 - val_loss: 0.0248 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.02463\n",
      "Epoch 113/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.0250 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02463\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0247 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.02463\n",
      "Epoch 115/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0109 - acc: 0.9972 - val_loss: 0.0242 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.02463 to 0.02421, saving model to best_model_whistle.hdf5\n",
      "Epoch 116/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0121 - acc: 0.9966 - val_loss: 0.0254 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02421\n",
      "Epoch 117/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.0247 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.02421\n",
      "Epoch 118/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0102 - acc: 0.9978 - val_loss: 0.0261 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.02421\n",
      "Epoch 119/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.0248 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.02421\n",
      "Epoch 120/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0103 - acc: 0.9975 - val_loss: 0.0240 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.02421 to 0.02399, saving model to best_model_whistle.hdf5\n",
      "Epoch 121/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.0229 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02399 to 0.02294, saving model to best_model_whistle.hdf5\n",
      "Epoch 122/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.0254 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.02294\n",
      "Epoch 123/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0223 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.02294 to 0.02229, saving model to best_model_whistle.hdf5\n",
      "Epoch 124/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0103 - acc: 0.9971 - val_loss: 0.0246 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.02229\n",
      "Epoch 125/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0222 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.02229 to 0.02217, saving model to best_model_whistle.hdf5\n",
      "Epoch 126/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0362 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.02217\n",
      "Epoch 127/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0093 - acc: 0.9978 - val_loss: 0.0221 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02217 to 0.02212, saving model to best_model_whistle.hdf5\n",
      "Epoch 128/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0096 - acc: 0.9975 - val_loss: 0.0253 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.02212\n",
      "Epoch 129/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0094 - acc: 0.9978 - val_loss: 0.0219 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.02212 to 0.02188, saving model to best_model_whistle.hdf5\n",
      "Epoch 130/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0091 - acc: 0.9979 - val_loss: 0.0230 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.02188\n",
      "Epoch 131/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0092 - acc: 0.9979 - val_loss: 0.0229 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.02188\n",
      "Epoch 132/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0092 - acc: 0.9981 - val_loss: 0.0223 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.02188\n",
      "Epoch 133/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0087 - acc: 0.9980 - val_loss: 0.0231 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.02188\n",
      "Epoch 134/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0088 - acc: 0.9977 - val_loss: 0.0228 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.02188\n",
      "Epoch 135/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0087 - acc: 0.9979 - val_loss: 0.0215 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.02188 to 0.02153, saving model to best_model_whistle.hdf5\n",
      "Epoch 136/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0236 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.02153\n",
      "Epoch 137/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0090 - acc: 0.9979 - val_loss: 0.0234 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.02153\n",
      "Epoch 138/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0250 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.02153\n",
      "Epoch 139/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0210 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.02153 to 0.02097, saving model to best_model_whistle.hdf5\n",
      "Epoch 140/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0080 - acc: 0.9983 - val_loss: 0.0246 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.02097\n",
      "Epoch 141/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0078 - acc: 0.9987 - val_loss: 0.0205 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.02097 to 0.02047, saving model to best_model_whistle.hdf5\n",
      "Epoch 142/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0085 - acc: 0.9980 - val_loss: 0.0198 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.02047 to 0.01976, saving model to best_model_whistle.hdf5\n",
      "Epoch 143/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0196 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.01976 to 0.01962, saving model to best_model_whistle.hdf5\n",
      "Epoch 144/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0077 - acc: 0.9986 - val_loss: 0.0205 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01962\n",
      "Epoch 145/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0206 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01962\n",
      "Epoch 146/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0273 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01962\n",
      "Epoch 147/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.0281 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01962\n",
      "Epoch 148/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0075 - acc: 0.9985 - val_loss: 0.0234 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.01962\n",
      "Epoch 149/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0071 - acc: 0.9987 - val_loss: 0.0226 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01962\n",
      "Epoch 150/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0213 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01962\n",
      "Epoch 151/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0071 - acc: 0.9985 - val_loss: 0.0202 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01962\n",
      "Epoch 152/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0203 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01962\n",
      "Epoch 153/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.0189 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.01962 to 0.01890, saving model to best_model_whistle.hdf5\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0214 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.01890\n",
      "Epoch 155/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 0.0196 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01890\n",
      "Epoch 156/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0071 - acc: 0.9983 - val_loss: 0.0196 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01890\n",
      "Epoch 157/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0066 - acc: 0.9987 - val_loss: 0.0215 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.01890\n",
      "Epoch 158/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0201 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01890\n",
      "Epoch 159/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0067 - acc: 0.9986 - val_loss: 0.0254 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01890\n",
      "Epoch 160/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.0191 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.01890\n",
      "Epoch 161/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.0193 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.01890\n",
      "Epoch 162/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0186 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.01890 to 0.01862, saving model to best_model_whistle.hdf5\n",
      "Epoch 163/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0060 - acc: 0.9989 - val_loss: 0.0186 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01862\n",
      "Epoch 164/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0184 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.01862 to 0.01841, saving model to best_model_whistle.hdf5\n",
      "Epoch 165/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0183 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.01841 to 0.01832, saving model to best_model_whistle.hdf5\n",
      "Epoch 166/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0190 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01832\n",
      "Epoch 167/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0198 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.01832\n",
      "Epoch 168/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0208 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.01832\n",
      "Epoch 169/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0195 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01832\n",
      "Epoch 170/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0056 - acc: 0.9989 - val_loss: 0.0192 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.01832\n",
      "Epoch 171/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0195 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.01832\n",
      "Epoch 172/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0187 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01832\n",
      "Epoch 173/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0185 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.01832\n",
      "Epoch 174/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0204 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.01832\n",
      "Epoch 175/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.0176 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.01832 to 0.01760, saving model to best_model_whistle.hdf5\n",
      "Epoch 176/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.0177 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.01760\n",
      "Epoch 177/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 0.0182 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01760\n",
      "Epoch 178/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.0176 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01760\n",
      "Epoch 179/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0055 - acc: 0.9992 - val_loss: 0.0174 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.01760 to 0.01744, saving model to best_model_whistle.hdf5\n",
      "Epoch 180/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0052 - acc: 0.9993 - val_loss: 0.0194 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01744\n",
      "Epoch 181/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0054 - acc: 0.9992 - val_loss: 0.0186 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01744\n",
      "Epoch 182/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 0.0172 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01744 to 0.01724, saving model to best_model_whistle.hdf5\n",
      "Epoch 183/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.0181 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01724\n",
      "Epoch 184/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 0.0172 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.01724 to 0.01723, saving model to best_model_whistle.hdf5\n",
      "Epoch 185/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0171 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.01723 to 0.01712, saving model to best_model_whistle.hdf5\n",
      "Epoch 186/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.0175 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01712\n",
      "Epoch 187/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0173 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01712\n",
      "Epoch 188/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0047 - acc: 0.9992 - val_loss: 0.0209 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01712\n",
      "Epoch 189/200\n",
      "12012/12012 [==============================] - 0s 40us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0188 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01712\n",
      "Epoch 190/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0189 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01712\n",
      "Epoch 191/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.0188 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01712\n",
      "Epoch 192/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 0.0188 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01712\n",
      "Epoch 193/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0174 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01712\n",
      "Epoch 194/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.0157 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.01712 to 0.01568, saving model to best_model_whistle.hdf5\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0159 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.01568\n",
      "Epoch 196/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.0225 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01568\n",
      "Epoch 197/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.0175 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.01568\n",
      "Epoch 198/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0176 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01568\n",
      "Epoch 199/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0043 - acc: 0.9995 - val_loss: 0.0168 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01568\n",
      "Epoch 200/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.0220 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01568\n",
      "Train on 12012 samples, validate on 3003 samples\n",
      "Epoch 1/200\n",
      "12012/12012 [==============================] - 1s 42us/step - loss: 0.3153 - acc: 0.8854 - val_loss: 0.1834 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18337, saving model to best_model_whistle.hdf5\n",
      "Epoch 2/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.1667 - acc: 0.9396 - val_loss: 0.1375 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18337 to 0.13748, saving model to best_model_whistle.hdf5\n",
      "Epoch 3/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.1345 - acc: 0.9506 - val_loss: 0.1197 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13748 to 0.11970, saving model to best_model_whistle.hdf5\n",
      "Epoch 4/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.1214 - acc: 0.9558 - val_loss: 0.1110 - val_acc: 0.9620\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11970 to 0.11103, saving model to best_model_whistle.hdf5\n",
      "Epoch 5/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.1107 - acc: 0.9591 - val_loss: 0.1033 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11103 to 0.10328, saving model to best_model_whistle.hdf5\n",
      "Epoch 6/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.1039 - acc: 0.9612 - val_loss: 0.1017 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10328 to 0.10167, saving model to best_model_whistle.hdf5\n",
      "Epoch 7/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0975 - acc: 0.9647 - val_loss: 0.0965 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10167 to 0.09652, saving model to best_model_whistle.hdf5\n",
      "Epoch 8/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0933 - acc: 0.9651 - val_loss: 0.0938 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09652 to 0.09379, saving model to best_model_whistle.hdf5\n",
      "Epoch 9/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0891 - acc: 0.9678 - val_loss: 0.0923 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09379 to 0.09225, saving model to best_model_whistle.hdf5\n",
      "Epoch 10/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0848 - acc: 0.9690 - val_loss: 0.0840 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09225 to 0.08398, saving model to best_model_whistle.hdf5\n",
      "Epoch 11/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0828 - acc: 0.9694 - val_loss: 0.0800 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08398 to 0.07997, saving model to best_model_whistle.hdf5\n",
      "Epoch 12/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0787 - acc: 0.9710 - val_loss: 0.0802 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07997\n",
      "Epoch 13/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0767 - acc: 0.9720 - val_loss: 0.0742 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.07997 to 0.07417, saving model to best_model_whistle.hdf5\n",
      "Epoch 14/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0743 - acc: 0.9724 - val_loss: 0.0720 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07417 to 0.07203, saving model to best_model_whistle.hdf5\n",
      "Epoch 15/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0703 - acc: 0.9729 - val_loss: 0.0698 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07203 to 0.06980, saving model to best_model_whistle.hdf5\n",
      "Epoch 16/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0681 - acc: 0.9746 - val_loss: 0.0675 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06980 to 0.06752, saving model to best_model_whistle.hdf5\n",
      "Epoch 17/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0661 - acc: 0.9753 - val_loss: 0.0671 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06752 to 0.06710, saving model to best_model_whistle.hdf5\n",
      "Epoch 18/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0636 - acc: 0.9760 - val_loss: 0.0641 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06710 to 0.06414, saving model to best_model_whistle.hdf5\n",
      "Epoch 19/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0613 - acc: 0.9769 - val_loss: 0.0624 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06414 to 0.06238, saving model to best_model_whistle.hdf5\n",
      "Epoch 20/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0593 - acc: 0.9774 - val_loss: 0.0614 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06238 to 0.06145, saving model to best_model_whistle.hdf5\n",
      "Epoch 21/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0593 - acc: 0.9770 - val_loss: 0.0598 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06145 to 0.05978, saving model to best_model_whistle.hdf5\n",
      "Epoch 22/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0561 - acc: 0.9786 - val_loss: 0.0594 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05978 to 0.05941, saving model to best_model_whistle.hdf5\n",
      "Epoch 23/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0544 - acc: 0.9798 - val_loss: 0.0559 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05941 to 0.05592, saving model to best_model_whistle.hdf5\n",
      "Epoch 24/200\n",
      "12012/12012 [==============================] - 1s 42us/step - loss: 0.0521 - acc: 0.9801 - val_loss: 0.0544 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05592 to 0.05438, saving model to best_model_whistle.hdf5\n",
      "Epoch 25/200\n",
      "12012/12012 [==============================] - 0s 40us/step - loss: 0.0510 - acc: 0.9804 - val_loss: 0.0582 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05438\n",
      "Epoch 26/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0514 - acc: 0.9796 - val_loss: 0.0536 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05438 to 0.05361, saving model to best_model_whistle.hdf5\n",
      "Epoch 27/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0489 - acc: 0.9814 - val_loss: 0.0518 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05361 to 0.05179, saving model to best_model_whistle.hdf5\n",
      "Epoch 28/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0469 - acc: 0.9824 - val_loss: 0.0494 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05179 to 0.04936, saving model to best_model_whistle.hdf5\n",
      "Epoch 29/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0462 - acc: 0.9824 - val_loss: 0.0534 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04936\n",
      "Epoch 30/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0444 - acc: 0.9833 - val_loss: 0.0475 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04936 to 0.04748, saving model to best_model_whistle.hdf5\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0435 - acc: 0.9842 - val_loss: 0.0463 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04748 to 0.04632, saving model to best_model_whistle.hdf5\n",
      "Epoch 32/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0431 - acc: 0.9838 - val_loss: 0.0449 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04632 to 0.04487, saving model to best_model_whistle.hdf5\n",
      "Epoch 33/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0406 - acc: 0.9853 - val_loss: 0.0443 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04487 to 0.04430, saving model to best_model_whistle.hdf5\n",
      "Epoch 34/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0402 - acc: 0.9855 - val_loss: 0.0438 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04430 to 0.04381, saving model to best_model_whistle.hdf5\n",
      "Epoch 35/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0393 - acc: 0.9862 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04381 to 0.04222, saving model to best_model_whistle.hdf5\n",
      "Epoch 36/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0372 - acc: 0.9870 - val_loss: 0.0457 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.04222\n",
      "Epoch 37/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0372 - acc: 0.9866 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.04222\n",
      "Epoch 38/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0354 - acc: 0.9873 - val_loss: 0.0398 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04222 to 0.03983, saving model to best_model_whistle.hdf5\n",
      "Epoch 39/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0350 - acc: 0.9878 - val_loss: 0.0390 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.03983 to 0.03900, saving model to best_model_whistle.hdf5\n",
      "Epoch 40/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0345 - acc: 0.9885 - val_loss: 0.0386 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03900 to 0.03864, saving model to best_model_whistle.hdf5\n",
      "Epoch 41/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0337 - acc: 0.9888 - val_loss: 0.0379 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03864 to 0.03785, saving model to best_model_whistle.hdf5\n",
      "Epoch 42/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0336 - acc: 0.9888 - val_loss: 0.0413 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03785\n",
      "Epoch 43/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0332 - acc: 0.9880 - val_loss: 0.0376 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.03785 to 0.03762, saving model to best_model_whistle.hdf5\n",
      "Epoch 44/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0316 - acc: 0.9892 - val_loss: 0.0377 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03762\n",
      "Epoch 45/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0310 - acc: 0.9888 - val_loss: 0.0370 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03762 to 0.03700, saving model to best_model_whistle.hdf5\n",
      "Epoch 46/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0298 - acc: 0.9893 - val_loss: 0.0435 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03700\n",
      "Epoch 47/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0292 - acc: 0.9897 - val_loss: 0.0380 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03700\n",
      "Epoch 48/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0288 - acc: 0.9900 - val_loss: 0.0344 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03700 to 0.03440, saving model to best_model_whistle.hdf5\n",
      "Epoch 49/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0292 - acc: 0.9896 - val_loss: 0.0353 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03440\n",
      "Epoch 50/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0277 - acc: 0.9908 - val_loss: 0.0339 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03440 to 0.03387, saving model to best_model_whistle.hdf5\n",
      "Epoch 51/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0268 - acc: 0.9903 - val_loss: 0.0398 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03387\n",
      "Epoch 52/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0267 - acc: 0.9909 - val_loss: 0.0332 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03387 to 0.03321, saving model to best_model_whistle.hdf5\n",
      "Epoch 53/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0261 - acc: 0.9909 - val_loss: 0.0326 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03321 to 0.03261, saving model to best_model_whistle.hdf5\n",
      "Epoch 54/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0270 - acc: 0.9904 - val_loss: 0.0392 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03261\n",
      "Epoch 55/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0264 - acc: 0.9912 - val_loss: 0.0338 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03261\n",
      "Epoch 56/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0247 - acc: 0.9916 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03261\n",
      "Epoch 57/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0243 - acc: 0.9918 - val_loss: 0.0306 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.03261 to 0.03058, saving model to best_model_whistle.hdf5\n",
      "Epoch 58/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0242 - acc: 0.9909 - val_loss: 0.0302 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.03058 to 0.03018, saving model to best_model_whistle.hdf5\n",
      "Epoch 59/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0235 - acc: 0.9923 - val_loss: 0.0299 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03018 to 0.02987, saving model to best_model_whistle.hdf5\n",
      "Epoch 60/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0237 - acc: 0.9918 - val_loss: 0.0294 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02987 to 0.02941, saving model to best_model_whistle.hdf5\n",
      "Epoch 61/200\n",
      "12012/12012 [==============================] - 1s 42us/step - loss: 0.0224 - acc: 0.9924 - val_loss: 0.0295 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02941\n",
      "Epoch 62/200\n",
      "12012/12012 [==============================] - 1s 42us/step - loss: 0.0221 - acc: 0.9929 - val_loss: 0.0298 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02941\n",
      "Epoch 63/200\n",
      "12012/12012 [==============================] - 1s 45us/step - loss: 0.0222 - acc: 0.9928 - val_loss: 0.0290 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02941 to 0.02897, saving model to best_model_whistle.hdf5\n",
      "Epoch 64/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0208 - acc: 0.9933 - val_loss: 0.0282 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02897 to 0.02823, saving model to best_model_whistle.hdf5\n",
      "Epoch 65/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0205 - acc: 0.9928 - val_loss: 0.0281 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.02823 to 0.02813, saving model to best_model_whistle.hdf5\n",
      "Epoch 66/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0285 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02813\n",
      "Epoch 67/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0276 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.02813 to 0.02755, saving model to best_model_whistle.hdf5\n",
      "Epoch 68/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0314 - val_acc: 0.9853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02755\n",
      "Epoch 69/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0197 - acc: 0.9934 - val_loss: 0.0291 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02755\n",
      "Epoch 70/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0274 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.02755 to 0.02743, saving model to best_model_whistle.hdf5\n",
      "Epoch 71/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.0299 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02743\n",
      "Epoch 72/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0191 - acc: 0.9933 - val_loss: 0.0283 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02743\n",
      "Epoch 73/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0265 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.02743 to 0.02650, saving model to best_model_whistle.hdf5\n",
      "Epoch 74/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0187 - acc: 0.9937 - val_loss: 0.0263 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.02650 to 0.02631, saving model to best_model_whistle.hdf5\n",
      "Epoch 75/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0272 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02631\n",
      "Epoch 76/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0250 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.02631 to 0.02498, saving model to best_model_whistle.hdf5\n",
      "Epoch 77/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0172 - acc: 0.9940 - val_loss: 0.0246 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.02498 to 0.02464, saving model to best_model_whistle.hdf5\n",
      "Epoch 78/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0167 - acc: 0.9951 - val_loss: 0.0256 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02464\n",
      "Epoch 79/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0167 - acc: 0.9944 - val_loss: 0.0255 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02464\n",
      "Epoch 80/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0237 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02464 to 0.02369, saving model to best_model_whistle.hdf5\n",
      "Epoch 81/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0236 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.02369 to 0.02358, saving model to best_model_whistle.hdf5\n",
      "Epoch 82/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0151 - acc: 0.9951 - val_loss: 0.0230 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.02358 to 0.02298, saving model to best_model_whistle.hdf5\n",
      "Epoch 83/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.0231 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02298\n",
      "Epoch 84/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0254 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02298\n",
      "Epoch 85/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0145 - acc: 0.9958 - val_loss: 0.0245 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02298\n",
      "Epoch 86/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0143 - acc: 0.9957 - val_loss: 0.0229 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.02298 to 0.02286, saving model to best_model_whistle.hdf5\n",
      "Epoch 87/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0143 - acc: 0.9958 - val_loss: 0.0229 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02286\n",
      "Epoch 88/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0137 - acc: 0.9965 - val_loss: 0.0223 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.02286 to 0.02233, saving model to best_model_whistle.hdf5\n",
      "Epoch 89/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0233 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02233\n",
      "Epoch 90/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0212 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02233 to 0.02123, saving model to best_model_whistle.hdf5\n",
      "Epoch 91/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 0.0210 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02123 to 0.02098, saving model to best_model_whistle.hdf5\n",
      "Epoch 92/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0129 - acc: 0.9967 - val_loss: 0.0221 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02098\n",
      "Epoch 93/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0239 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02098\n",
      "Epoch 94/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0267 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02098\n",
      "Epoch 95/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0291 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02098\n",
      "Epoch 96/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0124 - acc: 0.9970 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02098\n",
      "Epoch 97/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0289 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02098\n",
      "Epoch 98/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0117 - acc: 0.9971 - val_loss: 0.0270 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02098\n",
      "Epoch 99/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0115 - acc: 0.9969 - val_loss: 0.0346 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02098\n",
      "Epoch 100/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 0.0220 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02098\n",
      "Epoch 101/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0109 - acc: 0.9973 - val_loss: 0.0202 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.02098 to 0.02016, saving model to best_model_whistle.hdf5\n",
      "Epoch 102/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0110 - acc: 0.9972 - val_loss: 0.0205 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.02016\n",
      "Epoch 103/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0110 - acc: 0.9973 - val_loss: 0.0200 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02016 to 0.02001, saving model to best_model_whistle.hdf5\n",
      "Epoch 104/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0226 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.02001\n",
      "Epoch 105/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0110 - acc: 0.9972 - val_loss: 0.0250 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02001\n",
      "Epoch 106/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.0187 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02001 to 0.01866, saving model to best_model_whistle.hdf5\n",
      "Epoch 107/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0100 - acc: 0.9978 - val_loss: 0.0322 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.01866\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.0199 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.01866\n",
      "Epoch 109/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0104 - acc: 0.9971 - val_loss: 0.0185 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.01866 to 0.01851, saving model to best_model_whistle.hdf5\n",
      "Epoch 110/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0105 - acc: 0.9975 - val_loss: 0.0189 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.01851\n",
      "Epoch 111/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0098 - acc: 0.9975 - val_loss: 0.0181 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.01851 to 0.01813, saving model to best_model_whistle.hdf5\n",
      "Epoch 112/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.0186 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.01813\n",
      "Epoch 113/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0098 - acc: 0.9978 - val_loss: 0.0177 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.01813 to 0.01766, saving model to best_model_whistle.hdf5\n",
      "Epoch 114/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0095 - acc: 0.9981 - val_loss: 0.0180 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.01766\n",
      "Epoch 115/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0090 - acc: 0.9977 - val_loss: 0.0178 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.01766\n",
      "Epoch 116/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0095 - acc: 0.9978 - val_loss: 0.0188 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.01766\n",
      "Epoch 117/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0195 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.01766\n",
      "Epoch 118/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0087 - acc: 0.9979 - val_loss: 0.0181 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.01766\n",
      "Epoch 119/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0085 - acc: 0.9985 - val_loss: 0.0168 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.01766 to 0.01677, saving model to best_model_whistle.hdf5\n",
      "Epoch 120/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0089 - acc: 0.9982 - val_loss: 0.0220 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.01677\n",
      "Epoch 121/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0087 - acc: 0.9981 - val_loss: 0.0177 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.01677\n",
      "Epoch 122/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0088 - acc: 0.9981 - val_loss: 0.0199 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.01677\n",
      "Epoch 123/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0211 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.01677\n",
      "Epoch 124/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0079 - acc: 0.9983 - val_loss: 0.0198 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.01677\n",
      "Epoch 125/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0075 - acc: 0.9986 - val_loss: 0.0226 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.01677\n",
      "Epoch 126/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0284 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.01677\n",
      "Epoch 127/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0077 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.01677\n",
      "Epoch 128/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0072 - acc: 0.9989 - val_loss: 0.0176 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01677\n",
      "Epoch 129/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0075 - acc: 0.9983 - val_loss: 0.0158 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.01677 to 0.01578, saving model to best_model_whistle.hdf5\n",
      "Epoch 130/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0071 - acc: 0.9985 - val_loss: 0.0188 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01578\n",
      "Epoch 131/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0177 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01578\n",
      "Epoch 132/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.0162 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01578\n",
      "Epoch 133/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0070 - acc: 0.9989 - val_loss: 0.0153 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.01578 to 0.01527, saving model to best_model_whistle.hdf5\n",
      "Epoch 134/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0163 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01527\n",
      "Epoch 135/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0070 - acc: 0.9987 - val_loss: 0.0183 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01527\n",
      "Epoch 136/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.0185 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01527\n",
      "Epoch 137/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0062 - acc: 0.9989 - val_loss: 0.0209 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.01527\n",
      "Epoch 138/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0262 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01527\n",
      "Epoch 139/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0178 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.01527\n",
      "Epoch 140/200\n",
      "12012/12012 [==============================] - 0s 39us/step - loss: 0.0070 - acc: 0.9984 - val_loss: 0.0200 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01527\n",
      "Epoch 141/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0153 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.01527\n",
      "Epoch 142/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0185 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01527\n",
      "Epoch 143/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 0.0148 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.01527 to 0.01484, saving model to best_model_whistle.hdf5\n",
      "Epoch 144/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0158 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01484\n",
      "Epoch 145/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0202 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01484\n",
      "Epoch 146/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0217 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01484\n",
      "Epoch 147/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0056 - acc: 0.9992 - val_loss: 0.0226 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01484\n",
      "Epoch 148/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0057 - acc: 0.9989 - val_loss: 0.0263 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.01484\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 0.0286 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01484\n",
      "Epoch 150/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.0249 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01484\n",
      "Epoch 151/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0067 - acc: 0.9983 - val_loss: 0.0246 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01484\n",
      "Epoch 152/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0338 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01484\n",
      "Epoch 153/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.0332 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.01484\n",
      "Epoch 154/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.0164 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.01484\n",
      "Epoch 155/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0051 - acc: 0.9990 - val_loss: 0.0208 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01484\n",
      "Epoch 156/200\n",
      "12012/12012 [==============================] - 1s 56us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 0.0217 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01484\n",
      "Epoch 157/200\n",
      "12012/12012 [==============================] - 0s 41us/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.0225 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.01484\n",
      "Epoch 158/200\n",
      "12012/12012 [==============================] - 0s 39us/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.0289 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01484\n",
      "Epoch 159/200\n",
      "12012/12012 [==============================] - 0s 41us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0316 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01484\n",
      "Epoch 160/200\n",
      "12012/12012 [==============================] - 0s 39us/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0197 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.01484\n",
      "Epoch 161/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.0222 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.01484\n",
      "Epoch 162/200\n",
      "12012/12012 [==============================] - 1s 42us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.0222 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.01484\n",
      "Epoch 163/200\n",
      "12012/12012 [==============================] - 1s 43us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.0261 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01484\n",
      "Epoch 164/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0574 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.01484\n",
      "Epoch 165/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.01484 to 0.01308, saving model to best_model_whistle.hdf5\n",
      "Epoch 166/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0040 - acc: 0.9996 - val_loss: 0.0135 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01308\n",
      "Epoch 167/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0040 - acc: 0.9997 - val_loss: 0.0163 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.01308\n",
      "Epoch 168/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.0150 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.01308\n",
      "Epoch 169/200\n",
      "12012/12012 [==============================] - 0s 41us/step - loss: 0.0038 - acc: 0.9995 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01308\n",
      "Epoch 170/200\n",
      "12012/12012 [==============================] - 1s 43us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0130 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.01308 to 0.01303, saving model to best_model_whistle.hdf5\n",
      "Epoch 171/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0037 - acc: 0.9996 - val_loss: 0.0128 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.01303 to 0.01279, saving model to best_model_whistle.hdf5\n",
      "Epoch 172/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0149 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01279\n",
      "Epoch 173/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0038 - acc: 0.9997 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.01279 to 0.01278, saving model to best_model_whistle.hdf5\n",
      "Epoch 174/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0124 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.01278 to 0.01243, saving model to best_model_whistle.hdf5\n",
      "Epoch 175/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0035 - acc: 0.9996 - val_loss: 0.0124 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.01243 to 0.01241, saving model to best_model_whistle.hdf5\n",
      "Epoch 176/200\n",
      "12012/12012 [==============================] - 1s 42us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0153 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.01241\n",
      "Epoch 177/200\n",
      "12012/12012 [==============================] - 0s 40us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 0.0151 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01241\n",
      "Epoch 178/200\n",
      "12012/12012 [==============================] - 0s 39us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 0.0140 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01241\n",
      "Epoch 179/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0114 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.01241 to 0.01138, saving model to best_model_whistle.hdf5\n",
      "Epoch 180/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.0412 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01138\n",
      "Epoch 181/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0036 - acc: 0.9995 - val_loss: 0.0146 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01138\n",
      "Epoch 182/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 0.0125 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.01138\n",
      "Epoch 183/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 0.0168 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01138\n",
      "Epoch 184/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 0.0161 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01138\n",
      "Epoch 185/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.0185 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01138\n",
      "Epoch 186/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0216 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01138\n",
      "Epoch 187/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0038 - acc: 0.9997 - val_loss: 0.0205 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01138\n",
      "Epoch 188/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.0248 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01138\n",
      "Epoch 189/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.0203 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01138\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0226 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01138\n",
      "Epoch 191/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.0251 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01138\n",
      "Epoch 192/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.0185 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01138\n",
      "Epoch 193/200\n",
      "12012/12012 [==============================] - 0s 39us/step - loss: 0.0028 - acc: 0.9996 - val_loss: 0.0240 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01138\n",
      "Epoch 194/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0027 - acc: 0.9998 - val_loss: 0.0244 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.01138\n",
      "Epoch 195/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0030 - acc: 0.9997 - val_loss: 0.0271 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.01138\n",
      "Epoch 196/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.0252 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01138\n",
      "Epoch 197/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.0317 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.01138\n",
      "Epoch 198/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01138\n",
      "Epoch 199/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0025 - acc: 0.9998 - val_loss: 0.0124 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01138\n",
      "Epoch 200/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.0122 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01138\n",
      "Train on 12012 samples, validate on 3003 samples\n",
      "Epoch 1/200\n",
      "12012/12012 [==============================] - 0s 40us/step - loss: 0.3136 - acc: 0.8919 - val_loss: 0.2020 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20196, saving model to best_model_whistle.hdf5\n",
      "Epoch 2/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.1647 - acc: 0.9402 - val_loss: 0.1572 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20196 to 0.15717, saving model to best_model_whistle.hdf5\n",
      "Epoch 3/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1352 - acc: 0.9516 - val_loss: 0.1327 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15717 to 0.13272, saving model to best_model_whistle.hdf5\n",
      "Epoch 4/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.1214 - acc: 0.9568 - val_loss: 0.1158 - val_acc: 0.9557\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13272 to 0.11580, saving model to best_model_whistle.hdf5\n",
      "Epoch 5/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.1098 - acc: 0.9604 - val_loss: 0.1086 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11580 to 0.10861, saving model to best_model_whistle.hdf5\n",
      "Epoch 6/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1030 - acc: 0.9620 - val_loss: 0.1164 - val_acc: 0.9550\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10861\n",
      "Epoch 7/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.1001 - acc: 0.9636 - val_loss: 0.1096 - val_acc: 0.9560\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10861\n",
      "Epoch 8/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0942 - acc: 0.9655 - val_loss: 0.0947 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10861 to 0.09467, saving model to best_model_whistle.hdf5\n",
      "Epoch 9/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0896 - acc: 0.9673 - val_loss: 0.0893 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09467 to 0.08928, saving model to best_model_whistle.hdf5\n",
      "Epoch 10/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0862 - acc: 0.9679 - val_loss: 0.0881 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08928 to 0.08807, saving model to best_model_whistle.hdf5\n",
      "Epoch 11/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0834 - acc: 0.9702 - val_loss: 0.0833 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08807 to 0.08328, saving model to best_model_whistle.hdf5\n",
      "Epoch 12/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0805 - acc: 0.9709 - val_loss: 0.0821 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08328 to 0.08212, saving model to best_model_whistle.hdf5\n",
      "Epoch 13/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0769 - acc: 0.9718 - val_loss: 0.0780 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08212 to 0.07795, saving model to best_model_whistle.hdf5\n",
      "Epoch 14/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0735 - acc: 0.9732 - val_loss: 0.0772 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07795 to 0.07715, saving model to best_model_whistle.hdf5\n",
      "Epoch 15/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0716 - acc: 0.9739 - val_loss: 0.0727 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07715 to 0.07270, saving model to best_model_whistle.hdf5\n",
      "Epoch 16/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0689 - acc: 0.9754 - val_loss: 0.0695 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07270 to 0.06952, saving model to best_model_whistle.hdf5\n",
      "Epoch 17/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0660 - acc: 0.9763 - val_loss: 0.0705 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06952\n",
      "Epoch 18/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0641 - acc: 0.9761 - val_loss: 0.0656 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06952 to 0.06562, saving model to best_model_whistle.hdf5\n",
      "Epoch 19/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0624 - acc: 0.9766 - val_loss: 0.0641 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06562 to 0.06406, saving model to best_model_whistle.hdf5\n",
      "Epoch 20/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0593 - acc: 0.9785 - val_loss: 0.0695 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06406\n",
      "Epoch 21/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0582 - acc: 0.9789 - val_loss: 0.0601 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06406 to 0.06008, saving model to best_model_whistle.hdf5\n",
      "Epoch 22/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0558 - acc: 0.9790 - val_loss: 0.0657 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06008\n",
      "Epoch 23/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0547 - acc: 0.9799 - val_loss: 0.0571 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06008 to 0.05710, saving model to best_model_whistle.hdf5\n",
      "Epoch 24/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0535 - acc: 0.9799 - val_loss: 0.0557 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05710 to 0.05568, saving model to best_model_whistle.hdf5\n",
      "Epoch 25/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0521 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05568\n",
      "Epoch 26/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0503 - acc: 0.9825 - val_loss: 0.0534 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05568 to 0.05336, saving model to best_model_whistle.hdf5\n",
      "Epoch 27/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0496 - acc: 0.9816 - val_loss: 0.0528 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05336 to 0.05282, saving model to best_model_whistle.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0477 - acc: 0.9820 - val_loss: 0.0594 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05282\n",
      "Epoch 29/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0466 - acc: 0.9826 - val_loss: 0.0500 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05282 to 0.05005, saving model to best_model_whistle.hdf5\n",
      "Epoch 30/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0456 - acc: 0.9833 - val_loss: 0.0488 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05005 to 0.04878, saving model to best_model_whistle.hdf5\n",
      "Epoch 31/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0446 - acc: 0.9832 - val_loss: 0.0466 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04878 to 0.04661, saving model to best_model_whistle.hdf5\n",
      "Epoch 32/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0426 - acc: 0.9846 - val_loss: 0.0456 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04661 to 0.04564, saving model to best_model_whistle.hdf5\n",
      "Epoch 33/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0425 - acc: 0.9842 - val_loss: 0.0457 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04564\n",
      "Epoch 34/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0410 - acc: 0.9858 - val_loss: 0.0464 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04564\n",
      "Epoch 35/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0401 - acc: 0.9855 - val_loss: 0.0446 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04564 to 0.04464, saving model to best_model_whistle.hdf5\n",
      "Epoch 36/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0395 - acc: 0.9862 - val_loss: 0.0442 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04464 to 0.04419, saving model to best_model_whistle.hdf5\n",
      "Epoch 37/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0388 - acc: 0.9852 - val_loss: 0.0421 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04419 to 0.04211, saving model to best_model_whistle.hdf5\n",
      "Epoch 38/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0382 - acc: 0.9866 - val_loss: 0.0421 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04211\n",
      "Epoch 39/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0374 - acc: 0.9862 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04211 to 0.04149, saving model to best_model_whistle.hdf5\n",
      "Epoch 40/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0374 - acc: 0.9860 - val_loss: 0.0425 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.04149\n",
      "Epoch 41/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0367 - acc: 0.9868 - val_loss: 0.0432 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.04149\n",
      "Epoch 42/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0350 - acc: 0.9873 - val_loss: 0.0454 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.04149\n",
      "Epoch 43/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0345 - acc: 0.9869 - val_loss: 0.0383 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04149 to 0.03827, saving model to best_model_whistle.hdf5\n",
      "Epoch 44/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0344 - acc: 0.9868 - val_loss: 0.0382 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03827 to 0.03822, saving model to best_model_whistle.hdf5\n",
      "Epoch 45/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0332 - acc: 0.9873 - val_loss: 0.0396 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03822\n",
      "Epoch 46/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0327 - acc: 0.9866 - val_loss: 0.0367 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03822 to 0.03673, saving model to best_model_whistle.hdf5\n",
      "Epoch 47/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0331 - acc: 0.9872 - val_loss: 0.0364 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03673 to 0.03641, saving model to best_model_whistle.hdf5\n",
      "Epoch 48/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0320 - acc: 0.9882 - val_loss: 0.0366 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03641\n",
      "Epoch 49/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0311 - acc: 0.9881 - val_loss: 0.0372 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03641\n",
      "Epoch 50/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0306 - acc: 0.9888 - val_loss: 0.0382 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03641\n",
      "Epoch 51/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0306 - acc: 0.9880 - val_loss: 0.0347 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03641 to 0.03471, saving model to best_model_whistle.hdf5\n",
      "Epoch 52/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0299 - acc: 0.9892 - val_loss: 0.0347 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03471 to 0.03465, saving model to best_model_whistle.hdf5\n",
      "Epoch 53/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0291 - acc: 0.9888 - val_loss: 0.0403 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03465\n",
      "Epoch 54/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0299 - acc: 0.9885 - val_loss: 0.0344 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03465 to 0.03436, saving model to best_model_whistle.hdf5\n",
      "Epoch 55/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0287 - acc: 0.9893 - val_loss: 0.0330 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.03436 to 0.03295, saving model to best_model_whistle.hdf5\n",
      "Epoch 56/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0282 - acc: 0.9893 - val_loss: 0.0389 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03295\n",
      "Epoch 57/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0280 - acc: 0.9898 - val_loss: 0.0315 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.03295 to 0.03148, saving model to best_model_whistle.hdf5\n",
      "Epoch 58/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0275 - acc: 0.9893 - val_loss: 0.0322 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03148\n",
      "Epoch 59/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0268 - acc: 0.9899 - val_loss: 0.0314 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03148 to 0.03142, saving model to best_model_whistle.hdf5\n",
      "Epoch 60/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0268 - acc: 0.9894 - val_loss: 0.0306 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03142 to 0.03060, saving model to best_model_whistle.hdf5\n",
      "Epoch 61/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0257 - acc: 0.9903 - val_loss: 0.0298 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03060 to 0.02977, saving model to best_model_whistle.hdf5\n",
      "Epoch 62/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0250 - acc: 0.9905 - val_loss: 0.0327 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02977\n",
      "Epoch 63/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0260 - acc: 0.9907 - val_loss: 0.0308 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02977\n",
      "Epoch 64/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0263 - acc: 0.9892 - val_loss: 0.0326 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02977\n",
      "Epoch 65/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0244 - acc: 0.9903 - val_loss: 0.0337 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02977\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0257 - acc: 0.9897 - val_loss: 0.0362 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02977\n",
      "Epoch 67/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0243 - acc: 0.9905 - val_loss: 0.0290 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.02977 to 0.02905, saving model to best_model_whistle.hdf5\n",
      "Epoch 68/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0228 - acc: 0.9906 - val_loss: 0.0310 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02905\n",
      "Epoch 69/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0234 - acc: 0.9906 - val_loss: 0.0298 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02905\n",
      "Epoch 70/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0230 - acc: 0.9918 - val_loss: 0.0291 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02905\n",
      "Epoch 71/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0229 - acc: 0.9904 - val_loss: 0.0344 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02905\n",
      "Epoch 72/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0239 - acc: 0.9907 - val_loss: 0.0300 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02905\n",
      "Epoch 73/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0223 - acc: 0.9916 - val_loss: 0.0292 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02905\n",
      "Epoch 74/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0219 - acc: 0.9920 - val_loss: 0.0287 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.02905 to 0.02872, saving model to best_model_whistle.hdf5\n",
      "Epoch 75/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0212 - acc: 0.9923 - val_loss: 0.0271 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.02872 to 0.02711, saving model to best_model_whistle.hdf5\n",
      "Epoch 76/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0213 - acc: 0.9915 - val_loss: 0.0267 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.02711 to 0.02672, saving model to best_model_whistle.hdf5\n",
      "Epoch 77/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0213 - acc: 0.9915 - val_loss: 0.0261 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.02672 to 0.02609, saving model to best_model_whistle.hdf5\n",
      "Epoch 78/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0210 - acc: 0.9917 - val_loss: 0.0292 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02609\n",
      "Epoch 79/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0211 - acc: 0.9926 - val_loss: 0.0283 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02609\n",
      "Epoch 80/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0198 - acc: 0.9925 - val_loss: 0.0259 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02609 to 0.02585, saving model to best_model_whistle.hdf5\n",
      "Epoch 81/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0200 - acc: 0.9923 - val_loss: 0.0306 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02585\n",
      "Epoch 82/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0195 - acc: 0.9921 - val_loss: 0.0284 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02585\n",
      "Epoch 83/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0194 - acc: 0.9925 - val_loss: 0.0248 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.02585 to 0.02480, saving model to best_model_whistle.hdf5\n",
      "Epoch 84/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0194 - acc: 0.9925 - val_loss: 0.0255 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02480\n",
      "Epoch 85/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0190 - acc: 0.9933 - val_loss: 0.0245 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.02480 to 0.02447, saving model to best_model_whistle.hdf5\n",
      "Epoch 86/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0190 - acc: 0.9930 - val_loss: 0.0248 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02447\n",
      "Epoch 87/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0182 - acc: 0.9933 - val_loss: 0.0258 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02447\n",
      "Epoch 88/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0179 - acc: 0.9938 - val_loss: 0.0240 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.02447 to 0.02404, saving model to best_model_whistle.hdf5\n",
      "Epoch 89/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0180 - acc: 0.9939 - val_loss: 0.0271 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02404\n",
      "Epoch 90/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0181 - acc: 0.9936 - val_loss: 0.0259 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02404\n",
      "Epoch 91/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0178 - acc: 0.9938 - val_loss: 0.0237 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02404 to 0.02373, saving model to best_model_whistle.hdf5\n",
      "Epoch 92/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0174 - acc: 0.9939 - val_loss: 0.0231 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02373 to 0.02306, saving model to best_model_whistle.hdf5\n",
      "Epoch 93/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0171 - acc: 0.9940 - val_loss: 0.0262 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02306\n",
      "Epoch 94/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0166 - acc: 0.9944 - val_loss: 0.0243 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02306\n",
      "Epoch 95/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0163 - acc: 0.9943 - val_loss: 0.0347 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02306\n",
      "Epoch 96/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0162 - acc: 0.9944 - val_loss: 0.0261 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02306\n",
      "Epoch 97/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0161 - acc: 0.9940 - val_loss: 0.0225 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02306 to 0.02247, saving model to best_model_whistle.hdf5\n",
      "Epoch 98/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0156 - acc: 0.9949 - val_loss: 0.0235 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02247\n",
      "Epoch 99/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0163 - acc: 0.9943 - val_loss: 0.0221 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.02247 to 0.02209, saving model to best_model_whistle.hdf5\n",
      "Epoch 100/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.0238 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02209\n",
      "Epoch 101/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0151 - acc: 0.9951 - val_loss: 0.0237 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.02209\n",
      "Epoch 102/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.0222 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.02209\n",
      "Epoch 103/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0155 - acc: 0.9949 - val_loss: 0.0234 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.02209\n",
      "Epoch 104/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0148 - acc: 0.9949 - val_loss: 0.0218 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.02209 to 0.02179, saving model to best_model_whistle.hdf5\n",
      "Epoch 105/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0248 - val_acc: 0.9913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02179\n",
      "Epoch 106/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0213 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02179 to 0.02134, saving model to best_model_whistle.hdf5\n",
      "Epoch 107/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0158 - acc: 0.9948 - val_loss: 0.0214 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.02134\n",
      "Epoch 108/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0138 - acc: 0.9955 - val_loss: 0.0236 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.02134\n",
      "Epoch 109/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0222 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.02134\n",
      "Epoch 110/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0136 - acc: 0.9950 - val_loss: 0.0208 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.02134 to 0.02082, saving model to best_model_whistle.hdf5\n",
      "Epoch 111/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0205 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.02082 to 0.02052, saving model to best_model_whistle.hdf5\n",
      "Epoch 112/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0209 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.02052\n",
      "Epoch 113/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0128 - acc: 0.9967 - val_loss: 0.0220 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02052\n",
      "Epoch 114/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.0208 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.02052\n",
      "Epoch 115/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0127 - acc: 0.9962 - val_loss: 0.0276 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.02052\n",
      "Epoch 116/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0260 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02052\n",
      "Epoch 117/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 0.0237 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.02052\n",
      "Epoch 118/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0242 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.02052\n",
      "Epoch 119/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.02052\n",
      "Epoch 120/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0224 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.02052\n",
      "Epoch 121/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.0236 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.02052\n",
      "Epoch 122/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.0250 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.02052\n",
      "Epoch 123/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.0245 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.02052\n",
      "Epoch 124/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.0243 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.02052\n",
      "Epoch 125/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0293 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.02052\n",
      "Epoch 126/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0393 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.02052\n",
      "Epoch 127/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0183 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02052 to 0.01831, saving model to best_model_whistle.hdf5\n",
      "Epoch 128/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01831\n",
      "Epoch 129/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0107 - acc: 0.9973 - val_loss: 0.0188 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01831\n",
      "Epoch 130/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0180 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.01831 to 0.01801, saving model to best_model_whistle.hdf5\n",
      "Epoch 131/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0105 - acc: 0.9971 - val_loss: 0.0200 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01801\n",
      "Epoch 132/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0178 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.01801 to 0.01783, saving model to best_model_whistle.hdf5\n",
      "Epoch 133/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0104 - acc: 0.9973 - val_loss: 0.0183 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01783\n",
      "Epoch 134/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0194 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01783\n",
      "Epoch 135/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0102 - acc: 0.9971 - val_loss: 0.0195 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01783\n",
      "Epoch 136/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0183 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01783\n",
      "Epoch 137/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0099 - acc: 0.9975 - val_loss: 0.0186 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.01783\n",
      "Epoch 138/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0097 - acc: 0.9977 - val_loss: 0.0209 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01783\n",
      "Epoch 139/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.01783 to 0.01727, saving model to best_model_whistle.hdf5\n",
      "Epoch 140/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0105 - acc: 0.9964 - val_loss: 0.0178 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01727\n",
      "Epoch 141/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0093 - acc: 0.9973 - val_loss: 0.0168 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.01727 to 0.01676, saving model to best_model_whistle.hdf5\n",
      "Epoch 142/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0093 - acc: 0.9978 - val_loss: 0.0196 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01676\n",
      "Epoch 143/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0096 - acc: 0.9977 - val_loss: 0.0191 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.01676\n",
      "Epoch 144/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0196 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01676\n",
      "Epoch 145/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0091 - acc: 0.9975 - val_loss: 0.0203 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01676\n",
      "Epoch 146/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.0200 - val_acc: 0.9930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01676\n",
      "Epoch 147/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0086 - acc: 0.9978 - val_loss: 0.0189 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01676\n",
      "Epoch 148/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0087 - acc: 0.9980 - val_loss: 0.0223 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.01676\n",
      "Epoch 149/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0090 - acc: 0.9978 - val_loss: 0.0301 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01676\n",
      "Epoch 150/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0090 - acc: 0.9978 - val_loss: 0.0189 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01676\n",
      "Epoch 151/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0085 - acc: 0.9977 - val_loss: 0.0179 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01676\n",
      "Epoch 152/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0084 - acc: 0.9977 - val_loss: 0.0194 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01676\n",
      "Epoch 153/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0173 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.01676\n",
      "Epoch 154/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0077 - acc: 0.9983 - val_loss: 0.0159 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.01676 to 0.01586, saving model to best_model_whistle.hdf5\n",
      "Epoch 155/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0162 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01586\n",
      "Epoch 156/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.0194 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01586\n",
      "Epoch 157/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0183 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.01586\n",
      "Epoch 158/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0193 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01586\n",
      "Epoch 159/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0184 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01586\n",
      "Epoch 160/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0086 - acc: 0.9978 - val_loss: 0.0173 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.01586\n",
      "Epoch 161/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.01586\n",
      "Epoch 162/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0076 - acc: 0.9984 - val_loss: 0.0166 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.01586\n",
      "Epoch 163/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0190 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01586\n",
      "Epoch 164/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0172 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.01586\n",
      "Epoch 165/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0177 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.01586\n",
      "Epoch 166/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0073 - acc: 0.9985 - val_loss: 0.0162 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01586\n",
      "Epoch 167/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 0.0175 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.01586\n",
      "Epoch 168/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0068 - acc: 0.9983 - val_loss: 0.0154 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.01586 to 0.01544, saving model to best_model_whistle.hdf5\n",
      "Epoch 169/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0067 - acc: 0.9987 - val_loss: 0.0218 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01544\n",
      "Epoch 170/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.0172 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.01544\n",
      "Epoch 171/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.0209 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.01544\n",
      "Epoch 172/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0097 - acc: 0.9978 - val_loss: 0.0187 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01544\n",
      "Epoch 173/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.0181 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.01544\n",
      "Epoch 174/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.0219 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.01544\n",
      "Epoch 175/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.0166 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.01544\n",
      "Epoch 176/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0151 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.01544 to 0.01508, saving model to best_model_whistle.hdf5\n",
      "Epoch 177/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.0152 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01508\n",
      "Epoch 178/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.0153 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01508\n",
      "Epoch 179/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0061 - acc: 0.9991 - val_loss: 0.0161 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.01508\n",
      "Epoch 180/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0061 - acc: 0.9991 - val_loss: 0.0160 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01508\n",
      "Epoch 181/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.0178 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01508\n",
      "Epoch 182/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0058 - acc: 0.9991 - val_loss: 0.0162 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.01508\n",
      "Epoch 183/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0146 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.01508 to 0.01459, saving model to best_model_whistle.hdf5\n",
      "Epoch 184/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0058 - acc: 0.9993 - val_loss: 0.0176 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01459\n",
      "Epoch 185/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01459\n",
      "Epoch 186/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0146 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01459\n",
      "Epoch 187/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0055 - acc: 0.9991 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01459\n",
      "Epoch 188/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0055 - acc: 0.9991 - val_loss: 0.0205 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01459\n",
      "Epoch 189/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0057 - acc: 0.9991 - val_loss: 0.0176 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01459\n",
      "Epoch 190/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.0164 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01459\n",
      "Epoch 191/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.0159 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01459\n",
      "Epoch 192/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0053 - acc: 0.9993 - val_loss: 0.0161 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01459\n",
      "Epoch 193/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.0229 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01459\n",
      "Epoch 194/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0148 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.01459\n",
      "Epoch 195/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0153 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.01459\n",
      "Epoch 196/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0212 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01459\n",
      "Epoch 197/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.01459\n",
      "Epoch 198/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.0182 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01459\n",
      "Epoch 199/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.0177 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01459\n",
      "Epoch 200/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.0177 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01459\n",
      "Train on 12012 samples, validate on 3003 samples\n",
      "Epoch 1/200\n",
      "12012/12012 [==============================] - 0s 41us/step - loss: 0.3143 - acc: 0.8881 - val_loss: 0.1880 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18801, saving model to best_model_whistle.hdf5\n",
      "Epoch 2/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1662 - acc: 0.9416 - val_loss: 0.1394 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18801 to 0.13938, saving model to best_model_whistle.hdf5\n",
      "Epoch 3/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.1341 - acc: 0.9530 - val_loss: 0.1184 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13938 to 0.11837, saving model to best_model_whistle.hdf5\n",
      "Epoch 4/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.1201 - acc: 0.9558 - val_loss: 0.1069 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11837 to 0.10693, saving model to best_model_whistle.hdf5\n",
      "Epoch 5/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.1104 - acc: 0.9591 - val_loss: 0.0995 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10693 to 0.09949, saving model to best_model_whistle.hdf5\n",
      "Epoch 6/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.1036 - acc: 0.9601 - val_loss: 0.0953 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09949 to 0.09527, saving model to best_model_whistle.hdf5\n",
      "Epoch 7/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0979 - acc: 0.9637 - val_loss: 0.0902 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09527 to 0.09024, saving model to best_model_whistle.hdf5\n",
      "Epoch 8/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0927 - acc: 0.9650 - val_loss: 0.0854 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09024 to 0.08539, saving model to best_model_whistle.hdf5\n",
      "Epoch 9/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0882 - acc: 0.9673 - val_loss: 0.0823 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08539 to 0.08232, saving model to best_model_whistle.hdf5\n",
      "Epoch 10/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0851 - acc: 0.9677 - val_loss: 0.0797 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08232 to 0.07974, saving model to best_model_whistle.hdf5\n",
      "Epoch 11/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0824 - acc: 0.9696 - val_loss: 0.0773 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07974 to 0.07728, saving model to best_model_whistle.hdf5\n",
      "Epoch 12/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0777 - acc: 0.9709 - val_loss: 0.0785 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07728\n",
      "Epoch 13/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0764 - acc: 0.9704 - val_loss: 0.0717 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.07728 to 0.07174, saving model to best_model_whistle.hdf5\n",
      "Epoch 14/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0733 - acc: 0.9719 - val_loss: 0.0690 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07174 to 0.06896, saving model to best_model_whistle.hdf5\n",
      "Epoch 15/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0701 - acc: 0.9736 - val_loss: 0.0697 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06896\n",
      "Epoch 16/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0693 - acc: 0.9744 - val_loss: 0.0742 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06896\n",
      "Epoch 17/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0668 - acc: 0.9744 - val_loss: 0.0666 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06896 to 0.06655, saving model to best_model_whistle.hdf5\n",
      "Epoch 18/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0648 - acc: 0.9760 - val_loss: 0.0673 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06655\n",
      "Epoch 19/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0625 - acc: 0.9760 - val_loss: 0.0609 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06655 to 0.06085, saving model to best_model_whistle.hdf5\n",
      "Epoch 20/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0608 - acc: 0.9774 - val_loss: 0.0621 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06085\n",
      "Epoch 21/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0595 - acc: 0.9774 - val_loss: 0.0622 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06085\n",
      "Epoch 22/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0575 - acc: 0.9780 - val_loss: 0.0590 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.06085 to 0.05897, saving model to best_model_whistle.hdf5\n",
      "Epoch 23/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0565 - acc: 0.9787 - val_loss: 0.0578 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05897 to 0.05776, saving model to best_model_whistle.hdf5\n",
      "Epoch 24/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0548 - acc: 0.9784 - val_loss: 0.0594 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.05776\n",
      "Epoch 25/200\n",
      "12012/12012 [==============================] - 0s 39us/step - loss: 0.0537 - acc: 0.9801 - val_loss: 0.0545 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.05776 to 0.05453, saving model to best_model_whistle.hdf5\n",
      "Epoch 26/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0523 - acc: 0.9803 - val_loss: 0.0534 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05453 to 0.05344, saving model to best_model_whistle.hdf5\n",
      "Epoch 27/200\n",
      "12012/12012 [==============================] - 0s 42us/step - loss: 0.0502 - acc: 0.9814 - val_loss: 0.0542 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05344\n",
      "Epoch 28/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0497 - acc: 0.9814 - val_loss: 0.0496 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05344 to 0.04959, saving model to best_model_whistle.hdf5\n",
      "Epoch 29/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0487 - acc: 0.9814 - val_loss: 0.0484 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.04959 to 0.04838, saving model to best_model_whistle.hdf5\n",
      "Epoch 30/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0480 - acc: 0.9819 - val_loss: 0.0491 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04838\n",
      "Epoch 31/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0473 - acc: 0.9827 - val_loss: 0.0515 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.04838\n",
      "Epoch 32/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0459 - acc: 0.9830 - val_loss: 0.0540 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04838\n",
      "Epoch 33/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0458 - acc: 0.9824 - val_loss: 0.0454 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04838 to 0.04544, saving model to best_model_whistle.hdf5\n",
      "Epoch 34/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0450 - acc: 0.9828 - val_loss: 0.0463 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04544\n",
      "Epoch 35/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0434 - acc: 0.9840 - val_loss: 0.0450 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04544 to 0.04499, saving model to best_model_whistle.hdf5\n",
      "Epoch 36/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0430 - acc: 0.9839 - val_loss: 0.0460 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.04499\n",
      "Epoch 37/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0424 - acc: 0.9848 - val_loss: 0.0445 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04499 to 0.04450, saving model to best_model_whistle.hdf5\n",
      "Epoch 38/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0411 - acc: 0.9843 - val_loss: 0.0435 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04450 to 0.04350, saving model to best_model_whistle.hdf5\n",
      "Epoch 39/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0405 - acc: 0.9849 - val_loss: 0.0412 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04350 to 0.04125, saving model to best_model_whistle.hdf5\n",
      "Epoch 40/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0405 - acc: 0.9847 - val_loss: 0.0414 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.04125\n",
      "Epoch 41/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0396 - acc: 0.9859 - val_loss: 0.0405 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04125 to 0.04051, saving model to best_model_whistle.hdf5\n",
      "Epoch 42/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0388 - acc: 0.9855 - val_loss: 0.0467 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.04051\n",
      "Epoch 43/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0378 - acc: 0.9859 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.04051\n",
      "Epoch 44/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0376 - acc: 0.9858 - val_loss: 0.0393 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04051 to 0.03926, saving model to best_model_whistle.hdf5\n",
      "Epoch 45/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0374 - acc: 0.9863 - val_loss: 0.0392 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03926 to 0.03923, saving model to best_model_whistle.hdf5\n",
      "Epoch 46/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0370 - acc: 0.9859 - val_loss: 0.0395 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03923\n",
      "Epoch 47/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0361 - acc: 0.9858 - val_loss: 0.0392 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03923 to 0.03917, saving model to best_model_whistle.hdf5\n",
      "Epoch 48/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0349 - acc: 0.9872 - val_loss: 0.0372 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03917 to 0.03721, saving model to best_model_whistle.hdf5\n",
      "Epoch 49/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0346 - acc: 0.9866 - val_loss: 0.0375 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03721\n",
      "Epoch 50/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0347 - acc: 0.9866 - val_loss: 0.0371 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03721 to 0.03708, saving model to best_model_whistle.hdf5\n",
      "Epoch 51/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0362 - acc: 0.9858 - val_loss: 0.0358 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03708 to 0.03576, saving model to best_model_whistle.hdf5\n",
      "Epoch 52/200\n",
      "12012/12012 [==============================] - 0s 40us/step - loss: 0.0333 - acc: 0.9862 - val_loss: 0.0374 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03576\n",
      "Epoch 53/200\n",
      "12012/12012 [==============================] - 0s 40us/step - loss: 0.0335 - acc: 0.9876 - val_loss: 0.0363 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03576\n",
      "Epoch 54/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0341 - acc: 0.9872 - val_loss: 0.0362 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03576\n",
      "Epoch 55/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0338 - acc: 0.9872 - val_loss: 0.0354 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.03576 to 0.03542, saving model to best_model_whistle.hdf5\n",
      "Epoch 56/200\n",
      "12012/12012 [==============================] - 0s 41us/step - loss: 0.0321 - acc: 0.9878 - val_loss: 0.0360 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03542\n",
      "Epoch 57/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0316 - acc: 0.9885 - val_loss: 0.0355 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03542\n",
      "Epoch 58/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0311 - acc: 0.9886 - val_loss: 0.0364 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03542\n",
      "Epoch 59/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0313 - acc: 0.9882 - val_loss: 0.0342 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03542 to 0.03425, saving model to best_model_whistle.hdf5\n",
      "Epoch 60/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0303 - acc: 0.9887 - val_loss: 0.0334 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03425 to 0.03339, saving model to best_model_whistle.hdf5\n",
      "Epoch 61/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0301 - acc: 0.9882 - val_loss: 0.0339 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03339\n",
      "Epoch 62/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0296 - acc: 0.9883 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03339\n",
      "Epoch 63/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0297 - acc: 0.9889 - val_loss: 0.0335 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03339\n",
      "Epoch 64/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0295 - acc: 0.9885 - val_loss: 0.0331 - val_acc: 0.9880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00064: val_loss improved from 0.03339 to 0.03310, saving model to best_model_whistle.hdf5\n",
      "Epoch 65/200\n",
      "12012/12012 [==============================] - 0s 36us/step - loss: 0.0328 - acc: 0.9887 - val_loss: 0.0320 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.03310 to 0.03197, saving model to best_model_whistle.hdf5\n",
      "Epoch 66/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0281 - acc: 0.9897 - val_loss: 0.0323 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03197\n",
      "Epoch 67/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0284 - acc: 0.9893 - val_loss: 0.0313 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03197 to 0.03128, saving model to best_model_whistle.hdf5\n",
      "Epoch 68/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0281 - acc: 0.9899 - val_loss: 0.0403 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03128\n",
      "Epoch 69/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0280 - acc: 0.9888 - val_loss: 0.0345 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03128\n",
      "Epoch 70/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0278 - acc: 0.9896 - val_loss: 0.0338 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03128\n",
      "Epoch 71/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0269 - acc: 0.9895 - val_loss: 0.0311 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03128 to 0.03113, saving model to best_model_whistle.hdf5\n",
      "Epoch 72/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0265 - acc: 0.9893 - val_loss: 0.0313 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03113\n",
      "Epoch 73/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0262 - acc: 0.9899 - val_loss: 0.0379 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03113\n",
      "Epoch 74/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0279 - acc: 0.9893 - val_loss: 0.0314 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03113\n",
      "Epoch 75/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0270 - acc: 0.9896 - val_loss: 0.0335 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03113\n",
      "Epoch 76/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0267 - acc: 0.9895 - val_loss: 0.0325 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03113\n",
      "Epoch 77/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0256 - acc: 0.9907 - val_loss: 0.0313 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03113\n",
      "Epoch 78/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0257 - acc: 0.9904 - val_loss: 0.0298 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.03113 to 0.02984, saving model to best_model_whistle.hdf5\n",
      "Epoch 79/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0250 - acc: 0.9907 - val_loss: 0.0308 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02984\n",
      "Epoch 80/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0253 - acc: 0.9901 - val_loss: 0.0291 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02984 to 0.02909, saving model to best_model_whistle.hdf5\n",
      "Epoch 81/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0242 - acc: 0.9913 - val_loss: 0.0291 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02909\n",
      "Epoch 82/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0244 - acc: 0.9913 - val_loss: 0.0302 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02909\n",
      "Epoch 83/200\n",
      "12012/12012 [==============================] - 0s 38us/step - loss: 0.0241 - acc: 0.9907 - val_loss: 0.0293 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02909\n",
      "Epoch 84/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0241 - acc: 0.9910 - val_loss: 0.0361 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02909\n",
      "Epoch 85/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0247 - acc: 0.9909 - val_loss: 0.0282 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.02909 to 0.02820, saving model to best_model_whistle.hdf5\n",
      "Epoch 86/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0234 - acc: 0.9918 - val_loss: 0.0313 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02820\n",
      "Epoch 87/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0258 - acc: 0.9905 - val_loss: 0.0285 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02820\n",
      "Epoch 88/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0232 - acc: 0.9918 - val_loss: 0.0289 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02820\n",
      "Epoch 89/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0222 - acc: 0.9924 - val_loss: 0.0301 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02820\n",
      "Epoch 90/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0229 - acc: 0.9913 - val_loss: 0.0303 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02820\n",
      "Epoch 91/200\n",
      "12012/12012 [==============================] - 0s 37us/step - loss: 0.0225 - acc: 0.9918 - val_loss: 0.0271 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02820 to 0.02714, saving model to best_model_whistle.hdf5\n",
      "Epoch 92/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0230 - acc: 0.9915 - val_loss: 0.0283 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02714\n",
      "Epoch 93/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0226 - acc: 0.9917 - val_loss: 0.0277 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02714\n",
      "Epoch 94/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0223 - acc: 0.9918 - val_loss: 0.0278 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02714\n",
      "Epoch 95/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0224 - acc: 0.9918 - val_loss: 0.0283 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02714\n",
      "Epoch 96/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0215 - acc: 0.9923 - val_loss: 0.0272 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02714\n",
      "Epoch 97/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0208 - acc: 0.9924 - val_loss: 0.0316 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02714\n",
      "Epoch 98/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0208 - acc: 0.9926 - val_loss: 0.0272 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02714\n",
      "Epoch 99/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0213 - acc: 0.9922 - val_loss: 0.0280 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02714\n",
      "Epoch 100/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0215 - acc: 0.9916 - val_loss: 0.0279 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02714\n",
      "Epoch 101/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0204 - acc: 0.9931 - val_loss: 0.0272 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.02714\n",
      "Epoch 102/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0202 - acc: 0.9930 - val_loss: 0.0268 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02714 to 0.02677, saving model to best_model_whistle.hdf5\n",
      "Epoch 103/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0198 - acc: 0.9928 - val_loss: 0.0260 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02677 to 0.02595, saving model to best_model_whistle.hdf5\n",
      "Epoch 104/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0206 - acc: 0.9928 - val_loss: 0.0270 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.02595\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0195 - acc: 0.9933 - val_loss: 0.0389 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02595\n",
      "Epoch 106/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0200 - acc: 0.9928 - val_loss: 0.0275 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.02595\n",
      "Epoch 107/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0193 - acc: 0.9933 - val_loss: 0.0261 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.02595\n",
      "Epoch 108/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0191 - acc: 0.9933 - val_loss: 0.0290 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.02595\n",
      "Epoch 109/200\n",
      "12012/12012 [==============================] - 0s 34us/step - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0291 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.02595\n",
      "Epoch 110/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0195 - acc: 0.9921 - val_loss: 0.0261 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.02595\n",
      "Epoch 111/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0186 - acc: 0.9930 - val_loss: 0.0281 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.02595\n",
      "Epoch 112/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0182 - acc: 0.9931 - val_loss: 0.0258 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02595 to 0.02575, saving model to best_model_whistle.hdf5\n",
      "Epoch 113/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0185 - acc: 0.9933 - val_loss: 0.0295 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02575\n",
      "Epoch 114/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0186 - acc: 0.9931 - val_loss: 0.0256 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.02575 to 0.02558, saving model to best_model_whistle.hdf5\n",
      "Epoch 115/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.0262 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.02558\n",
      "Epoch 116/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0180 - acc: 0.9938 - val_loss: 0.0260 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02558\n",
      "Epoch 117/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0189 - acc: 0.9932 - val_loss: 0.0295 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.02558\n",
      "Epoch 118/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0183 - acc: 0.9942 - val_loss: 0.0273 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.02558\n",
      "Epoch 119/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0178 - acc: 0.9942 - val_loss: 0.0311 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.02558\n",
      "Epoch 120/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0172 - acc: 0.9938 - val_loss: 0.0278 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.02558\n",
      "Epoch 121/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0171 - acc: 0.9938 - val_loss: 0.0247 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02558 to 0.02473, saving model to best_model_whistle.hdf5\n",
      "Epoch 122/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0245 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.02473 to 0.02454, saving model to best_model_whistle.hdf5\n",
      "Epoch 123/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0172 - acc: 0.9947 - val_loss: 0.0252 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.02454\n",
      "Epoch 124/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0170 - acc: 0.9935 - val_loss: 0.0249 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.02454\n",
      "Epoch 125/200\n",
      "12012/12012 [==============================] - 0s 27us/step - loss: 0.0166 - acc: 0.9942 - val_loss: 0.0300 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.02454\n",
      "Epoch 126/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0173 - acc: 0.9939 - val_loss: 0.0259 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.02454\n",
      "Epoch 127/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0258 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.02454\n",
      "Epoch 128/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0167 - acc: 0.9943 - val_loss: 0.0246 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.02454\n",
      "Epoch 129/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0164 - acc: 0.9941 - val_loss: 0.0246 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.02454\n",
      "Epoch 130/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0163 - acc: 0.9951 - val_loss: 0.0241 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.02454 to 0.02413, saving model to best_model_whistle.hdf5\n",
      "Epoch 131/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0169 - acc: 0.9940 - val_loss: 0.0233 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.02413 to 0.02333, saving model to best_model_whistle.hdf5\n",
      "Epoch 132/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0166 - acc: 0.9935 - val_loss: 0.0241 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.02333\n",
      "Epoch 133/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0153 - acc: 0.9946 - val_loss: 0.0307 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.02333\n",
      "Epoch 134/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0165 - acc: 0.9944 - val_loss: 0.0334 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.02333\n",
      "Epoch 135/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0158 - acc: 0.9944 - val_loss: 0.0272 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.02333\n",
      "Epoch 136/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0154 - acc: 0.9942 - val_loss: 0.0233 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.02333 to 0.02330, saving model to best_model_whistle.hdf5\n",
      "Epoch 137/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.0240 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.02330\n",
      "Epoch 138/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0153 - acc: 0.9951 - val_loss: 0.0272 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.02330\n",
      "Epoch 139/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0150 - acc: 0.9949 - val_loss: 0.0276 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.02330\n",
      "Epoch 140/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0149 - acc: 0.9948 - val_loss: 0.0224 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02330 to 0.02237, saving model to best_model_whistle.hdf5\n",
      "Epoch 141/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0151 - acc: 0.9949 - val_loss: 0.0232 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.02237\n",
      "Epoch 142/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0235 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.02237\n",
      "Epoch 143/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0152 - acc: 0.9948 - val_loss: 0.0265 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.02237\n",
      "Epoch 144/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.0285 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.02237\n",
      "Epoch 145/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0145 - acc: 0.9945 - val_loss: 0.0239 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.02237\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0140 - acc: 0.9949 - val_loss: 0.0235 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.02237\n",
      "Epoch 147/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0148 - acc: 0.9945 - val_loss: 0.0223 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.02237 to 0.02226, saving model to best_model_whistle.hdf5\n",
      "Epoch 148/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0227 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.02226\n",
      "Epoch 149/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0142 - acc: 0.9952 - val_loss: 0.0238 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.02226\n",
      "Epoch 150/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0136 - acc: 0.9960 - val_loss: 0.0225 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.02226\n",
      "Epoch 151/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0228 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.02226\n",
      "Epoch 152/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0232 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.02226\n",
      "Epoch 153/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0228 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.02226\n",
      "Epoch 154/200\n",
      "12012/12012 [==============================] - 0s 35us/step - loss: 0.0137 - acc: 0.9962 - val_loss: 0.0230 - val_acc: 0.9923: 0s - loss: 0.0142 - acc: 0.9\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.02226\n",
      "Epoch 155/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0224 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.02226\n",
      "Epoch 156/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0130 - acc: 0.9959 - val_loss: 0.0232 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.02226\n",
      "Epoch 157/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0160 - acc: 0.9948 - val_loss: 0.0254 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.02226\n",
      "Epoch 158/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0237 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.02226\n",
      "Epoch 159/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0123 - acc: 0.9961 - val_loss: 0.0232 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.02226\n",
      "Epoch 160/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0221 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.02226 to 0.02207, saving model to best_model_whistle.hdf5\n",
      "Epoch 161/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.0258 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.02207\n",
      "Epoch 162/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0120 - acc: 0.9959 - val_loss: 0.0256 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.02207\n",
      "Epoch 163/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0255 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.02207\n",
      "Epoch 164/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0125 - acc: 0.9955 - val_loss: 0.0226 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.02207\n",
      "Epoch 165/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0225 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.02207\n",
      "Epoch 166/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0273 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.02207\n",
      "Epoch 167/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 0.0242 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.02207\n",
      "Epoch 168/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0236 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.02207\n",
      "Epoch 169/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.0221 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.02207 to 0.02206, saving model to best_model_whistle.hdf5\n",
      "Epoch 170/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0115 - acc: 0.9959 - val_loss: 0.0229 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.02206\n",
      "Epoch 171/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0229 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.02206\n",
      "Epoch 172/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0212 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.02206 to 0.02122, saving model to best_model_whistle.hdf5\n",
      "Epoch 173/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0227 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.02122\n",
      "Epoch 174/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0120 - acc: 0.9963 - val_loss: 0.0262 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.02122\n",
      "Epoch 175/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0210 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.02122 to 0.02103, saving model to best_model_whistle.hdf5\n",
      "Epoch 176/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0225 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.02103\n",
      "Epoch 177/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0212 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.02103\n",
      "Epoch 178/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0228 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.02103\n",
      "Epoch 179/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0213 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.02103\n",
      "Epoch 180/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0228 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.02103\n",
      "Epoch 181/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0207 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.02103 to 0.02072, saving model to best_model_whistle.hdf5\n",
      "Epoch 182/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0106 - acc: 0.9969 - val_loss: 0.0214 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.02072\n",
      "Epoch 183/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.0229 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.02072\n",
      "Epoch 184/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.0302 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.02072\n",
      "Epoch 185/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0221 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.02072\n",
      "Epoch 186/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0231 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.02072\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0226 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.02072\n",
      "Epoch 188/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0237 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.02072\n",
      "Epoch 189/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0103 - acc: 0.9969 - val_loss: 0.0208 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.02072\n",
      "Epoch 190/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0098 - acc: 0.9971 - val_loss: 0.0212 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.02072\n",
      "Epoch 191/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0212 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.02072\n",
      "Epoch 192/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0247 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.02072\n",
      "Epoch 193/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0101 - acc: 0.9969 - val_loss: 0.0261 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.02072\n",
      "Epoch 194/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0098 - acc: 0.9972 - val_loss: 0.0217 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.02072\n",
      "Epoch 195/200\n",
      "12012/12012 [==============================] - 0s 30us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0214 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.02072\n",
      "Epoch 196/200\n",
      "12012/12012 [==============================] - 0s 32us/step - loss: 0.0099 - acc: 0.9973 - val_loss: 0.0255 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.02072\n",
      "Epoch 197/200\n",
      "12012/12012 [==============================] - 0s 33us/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0234 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.02072\n",
      "Epoch 198/200\n",
      "12012/12012 [==============================] - 0s 31us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0212 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.02072\n",
      "Epoch 199/200\n",
      "12012/12012 [==============================] - 0s 29us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0207 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.02072\n",
      "Epoch 200/200\n",
      "12012/12012 [==============================] - 0s 28us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0222 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.02072\n",
      "Accuracy on train data: [0.9995837211608887, 0.9999167323112488, 0.9997502565383911, 0.999417245388031, 0.9971694946289062] 0.9991674900054932\n",
      "Accuracy on test data: [0.9920079708099365, 0.9936729669570923, 0.9956709742546082, 0.9943389892578125, 0.9920079708099365] 0.9935397744178772\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Kfold implementation, written by Paul Boersma\n",
    "def kfold_index(df, k=5):\n",
    "    N = len(df)\n",
    "    minimum_number_of_points_per_slice = N // k\n",
    "    remaining_number_of_points = N % k\n",
    "    starting_point = 0\n",
    "    out = []\n",
    "    for islice in range(0, k):\n",
    "        end_point = starting_point + minimum_number_of_points_per_slice + ( islice < remaining_number_of_points )\n",
    "        out.append((starting_point, end_point))\n",
    "        starting_point = end_point\n",
    "    return out\n",
    "\n",
    "# Run kfold for given featureset\n",
    "def execute_kfold(version, feature_type, denoise, k=5):\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    cm = np.zeros((2,2), dtype=int)\n",
    "    df = build_features(version, feature_type, denoise)\n",
    "    X, Y = feature_target_split(df)\n",
    "    for start, end in kfold_index(df, k):\n",
    "        X_train = np.concatenate((X[:start], X[end:]))\n",
    "        Y_train = np.concatenate((Y[:start], Y[end:]))\n",
    "        X_test = X[start:end]\n",
    "        Y_test = Y[start:end]\n",
    "        \n",
    "        model = train(X_train, X_test, Y_train, Y_test, version, feature_type, denoise)\n",
    "        acc_train.append(model.evaluate(X_train, Y_train, verbose=0)[1])\n",
    "        acc_test.append(model.evaluate(X_test, Y_test, verbose=0)[1])\n",
    "        cm += confusion_matrix(Y_test, [1 if prediction > .5 else 0 for prediction in model.predict(X_test)[:,0]])\n",
    "        \n",
    "    print(\"Accuracy on train data:\", acc_train, np.mean(acc_train))\n",
    "    print(\"Accuracy on test data:\", acc_test, np.mean(acc_test))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[False, True])\n",
    "    disp.plot(cmap=\"Greens\", values_format=\"\")\n",
    "\n",
    "#execute_kfold(1, \"fft\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FORMAT = pyaudio.paFloat32      # audio format (bytes per sample?)\n",
    "CHANNELS = 1                    # single channel for microphone\n",
    "SR = 48000                      # samples per second\n",
    "CHUNK = int((SR / 1000) * STEP) # chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_sample():\n",
    "    WAVE_OUTPUT_FILENAME = \"noise_sample.wav\"\n",
    "    RECORD_SECONDS = 1\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=CHANNELS,\n",
    "        rate=SR,\n",
    "        input=True,\n",
    "        output=True,\n",
    "        frames_per_buffer=CHUNK\n",
    "    )\n",
    "\n",
    "    print(\"* recording denoise\", int(SR / CHUNK * RECORD_SECONDS))\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(SR / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"* done recording denoise\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(SR)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    # load file & meta data\n",
    "    sample, sample_rate = librosa.load(WAVE_OUTPUT_FILENAME, sr=None)\n",
    "    return nr.reduce_noise(y=sample,  y_noise=sample[:5000], sr=SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whistle likelihood: 0.0% 199\n",
      "finished recording\n"
     ]
    }
   ],
   "source": [
    "def live_detect(feature_type, version, denoise, record_seconds=10):\n",
    "    # load model\n",
    "    try:\n",
    "        model = load_model(\"model_v\" + str(version) + \"_\" + feature_type + \"_\" + str(denoise) + \".h5\")\n",
    "    except:\n",
    "        _, _, model = train_model(version, feature_type, denoise)\n",
    "    \n",
    "    # pyaudio class instance\n",
    "    p = pyaudio.PyAudio()\n",
    "    buffer = [False] * 10\n",
    "    \n",
    "    if denoise:\n",
    "        noise_sample = get_noise_sample()\n",
    "\n",
    "    # stream object to get data from microphone\n",
    "    stream = p.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=SR,\n",
    "        input=True,\n",
    "        output=True,\n",
    "        frames_per_buffer=CHUNK\n",
    "    )\n",
    "\n",
    "    print(\"recording...\", int(SR / CHUNK * record_seconds))\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(SR / CHUNK * record_seconds)):\n",
    "        # read chunk\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "        \n",
    "        # fetch features\n",
    "        sample = np.frombuffer(data, dtype=np.float32)\n",
    "        \n",
    "        if denoise == True:\n",
    "            sample = nr.reduce_noise(y=sample,  y_noise=noise_sample, sr=SR)\n",
    "            \n",
    "        # get features\n",
    "        if feature_type == \"fft\":\n",
    "            features = np.mean(np.abs(librosa.stft(sample, n_fft=512, hop_length=256, win_length=512)).T, axis=0)\n",
    "        elif feature_type == \"mfcc\":\n",
    "            features = np.mean(librosa.feature.mfcc(y=sample, sr=SR, n_mfcc=40).T, axis=0)\n",
    "        \n",
    "        # make prediction\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "        is_whistle = model.predict(features, verbose=0)[0][0]\n",
    "        \n",
    "        # print current certainty\n",
    "        buffer.pop(0)\n",
    "        buffer.append(is_whistle > .5)\n",
    "        clear_output(wait=True)\n",
    "        print(\"whistle likelihood:\", str((sum(buffer)/len(buffer)) * 100) + \"%\", i)\n",
    "        \n",
    "        \n",
    "    print(\"finished recording\")\n",
    "\n",
    "    # stop recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "        \n",
    "live_detect(\"mfcc\", 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
